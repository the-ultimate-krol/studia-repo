{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/the-ultimate-krol/studia-repo/blob/main/Wprowadzenie_do_transformers%C3%B3w_maskowanie_laboratorium_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bad4bfa-ebdd-4102-a75e-4c714e51d4ce",
      "metadata": {
        "id": "6bad4bfa-ebdd-4102-a75e-4c714e51d4ce"
      },
      "source": [
        "# Wprowadzenie do architektury transformatorów\n",
        "\n",
        "## W poprzednich odcinkach\n",
        "\n",
        "Obecnie najpopularniejsze modele przetwarzania języka naturalnego wykorzystują architekturę Transformers. Istnieje kilka bibliotek implementujących tę architekturę. Jednak w kontekście NLP najczęściej używane są transformatory Huggingface.\n",
        "\n",
        "Oprócz samego kodu źródłowego, biblioteka ta zawiera szereg innych elementów. Do najważniejszych z nich należą\n",
        "\n",
        "[models](https://huggingface.co/models) -  ogromna i stale rosnąca liczba gotowych modeli, które możemy wykorzystać do rozwiązywania wielu problemów w NLP (ale także w rozpoznawaniu mowy czy przetwarzaniu obrazów),\n",
        "[datasets](https://huggingface.co/datasets) - bardzo duży katalog przydatnych zbiorów danych, które możemy łatwo wykorzystać do trenowania własnych modeli NLP (i nie tylko).\n",
        "\n",
        "## Przygotowanie środowiska - jak zacząć z Google Colab\n",
        "\n",
        "Trenowanie modeli NLP wymaga dostępu do akceleratorów sprzętowych przyspieszających uczenie się sieci neuronowych. Jeśli nasz komputer nie jest wyposażony w GPU, możemy skorzystać ze środowiska Google Colab.\n",
        "\n",
        "W środowisku tym możemy wybrać akcelerator spośród GPU i TPU. Sprawdźmy, czy mamy dostęp do środowiska wyposażonego w akcelerator NVidia, wykonując poniższe polecenie:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/jasonmayes/headless-chrome-nvidia-t4-gpu-support.git\n",
        "!cd headless-chrome-nvidia-t4-gpu-support && chmod +x scriptyMcScriptFace.sh && ./scriptyMcScriptFace.sh"
      ],
      "metadata": {
        "id": "JiJhFZckvk10"
      },
      "id": "JiJhFZckvk10",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "566e7ecc-0521-4093-9cbd-84c20ac3b851",
      "metadata": {
        "id": "566e7ecc-0521-4093-9cbd-84c20ac3b851"
      },
      "source": [
        "Następnie zainstalujemy wszystkie niezbędne biblioteki. Oprócz samej biblioteki `transformers`, zainstalujemy również `datasets` - bibliotekę zarządzającą zestawami danych, bibliotekę definiującą wiele metryk używanych w algorytmach AI `evaluate` oraz dodatkowe narzędzia, takie jak `sacremoses` i `sentencepiece`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97c5e836-fb0f-4d6a-8f9d-efc5d5c83403",
      "metadata": {
        "id": "97c5e836-fb0f-4d6a-8f9d-efc5d5c83403"
      },
      "outputs": [],
      "source": [
        "!pip install transformers sacremoses datasets evaluate sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f7ecc9d-75dc-42d4-b54e-6da8dcfb0ba8",
      "metadata": {
        "id": "5f7ecc9d-75dc-42d4-b54e-6da8dcfb0ba8"
      },
      "source": [
        "Po zainstalowaniu niezbędnych bibliotek możemy korzystać ze wszystkich modeli i zbiorów danych zarejestrowanych w katalogu.\n",
        "\n",
        "Typowym sposobem wykorzystania dostępnych modeli jest:\n",
        "\n",
        "- korzystanie z gotowego modelu realizującego konkretne zadanie, np. [analiza wydźwięku](https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis) - model tego typu nie wymaga trenowania, wystarczy go uruchomić, aby uzyskać wynik klasyfikacji (można to zobaczyć w demo pod wskazanym linkiem),\n",
        "- wykorzystanie modelu bazowego, który został wytrenowany do konkretnego zadania; przykładem takiego modelu jest [HerBERT base](https://huggingface.co/allegro/herbert-base-cased), który został nauczony jako model językowy wykorzystujący maskowanie. Aby użyć go do konkretnego zadania, musimy wybrać dla niego \"głowicę klasyfikacyjną\" i przetrenować go na naszym własnym zbiorze danych.\n",
        "\n",
        "Modele tego rodzaju są różne i mogą być ładowane przy użyciu wspólnego interfejsu, ale najlepiej jest użyć jednej z wyspecjalizowanych klas, dostosowanej do danego zadania (https://boinc-ai.gitbook.io/transformers/api/main-classes/auto-classes/natural-language-processing/automodelformaskedlm). Zaczniemy od załadowania modelu bazowego BERT - jednego z najpopularniejszych modeli dla języka angielskiego. Użyjemy go do odgadnięcia brakujących słów w tekście. W tym celu użyjemy wywołania `AutoModelForMaskedLM`, co można zobaczyć po wywołaniu kodu:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aee1cd5c-3c4f-4f4f-80a4-2c0a90de9419",
      "metadata": {
        "id": "aee1cd5c-3c4f-4f4f-80a4-2c0a90de9419"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b372b422-42a9-4ef8-9c00-013fc343c8d0",
      "metadata": {
        "id": "b372b422-42a9-4ef8-9c00-013fc343c8d0"
      },
      "source": [
        "## Łączenie z Google Drive\n",
        "Ostatnim elementem przygotowań, który jest opcjonalny, jest dołączenie własnego Dysku Google do środowiska Colab. Umożliwia to zapisywanie wytrenowanych modeli podczas procesu szkolenia na \"zewnętrznym\" dysku. Jeśli Google Colab doprowadzi do przerwania procesu szkolenia, pliki, które zostały pomyślnie zapisane podczas szkolenia, nie zostaną utracone. Możliwe będzie wznowienie treningu już na częściowo wytrenowanym modelu.\n",
        "\n",
        "Aby to zrobić, montujemy Dysk Google w Colab. Wymaga to autoryzacji narzędzia Colab na Dysku Google."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a5a8a39-abb1-4994-a23d-15e911bf6319",
      "metadata": {
        "id": "8a5a8a39-abb1-4994-a23d-15e911bf6319"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01902f89-088c-4de9-9883-6aef8c65d769",
      "metadata": {
        "id": "01902f89-088c-4de9-9883-6aef8c65d769"
      },
      "source": [
        "Po zamontowaniu dysku mamy dostęp do całej zawartości Dysku Google. Wskazując, gdzie zapisać dane podczas treningu, należy wskazać ścieżkę zaczynającą się od `/content/gdrive` i wskazać jakiś podkatalog w naszej przestrzeni dyskowej. Pełną ścieżką może być `/content/gdrive/MyDrive/output`. Dobrym pomysłem jest sprawdzenie, czy dane są zapisywane na dysku przed uruchomieniem treningu.\n",
        "\n",
        "## Tokenizacja tekstu\n",
        "Samo załadowanie modelu nie wystarczy jednak, by zacząć z niego korzystać. Musimy dysponować mechanizmem konwersji tekstu (ciągu znaków), na sekwencję tokenów, należących do określonego słownika. Podczas uczenia modelu słownik ten jest określany (wybierany algorytmicznie) przed właściwym uczeniem sieci neuronowej. Chociaż możliwe jest jego późniejsze rozszerzenie (trening na danych treningowych; pozwala również uzyskać reprezentację brakujących tokenów), zwykle używany jest słownik w formie zdefiniowanej przed treningiem sieci neuronowej. Dlatego ważne jest, aby określić prawidłowy słownik dla tokenizera wykonującego dzielenie tekstu.\n",
        "\n",
        "Biblioteka posiada klasę `AutoTokenizer`, która akceptuje nazwę modelu, co pozwala na automatyczne załadowanie słownika odpowiadającego wybranemu modelowi sieci neuronowej. Należy jednak pamiętać, że w przypadku korzystania z dwóch modeli, każdy z nich najprawdopodobniej będzie miał inny słownik, a zatem muszą one mieć własne instancje klasy `Tokenizer` (https://huggingface.co/docs/transformers/main_classes/tokenizer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19523e67-58f5-42ae-8d57-52fe82469d69",
      "metadata": {
        "id": "19523e67-58f5-42ae-8d57-52fe82469d69"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59c6c47c-d333-45bf-b95a-2e3743ed9793",
      "metadata": {
        "id": "59c6c47c-d333-45bf-b95a-2e3743ed9793"
      },
      "source": [
        "Tokenizer używa słownika o stałym rozmiarze, co powoduje, że nie wszystkie słowa występujące w tekście zostaną uwzględnione. A jeśli użyjemy tokenizera do podzielenia tekstu w języku innym niż ten, dla którego został stworzony, taki tekst zostanie podzielony na większą liczbę tokenów. (https://huggingface.co/docs/transformers/main_classes/tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56cc1dbc-b6bd-4ac4-bbe5-63decb249538",
      "metadata": {
        "id": "56cc1dbc-b6bd-4ac4-bbe5-63decb249538"
      },
      "outputs": [],
      "source": [
        "sentence1 = tokenizer.encode(\n",
        "    \"The quick brown fox jumps over the lazy dog.\", return_tensors=\"pt\"\n",
        ")\n",
        "print(sentence1)\n",
        "print(sentence1.shape)\n",
        "\n",
        "sentence2 = tokenizer.encode(\"Zażółć gęślą jaźń.\", return_tensors=\"pt\")\n",
        "print(sentence2)\n",
        "print(sentence2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c82ecb1a-b415-4c43-9a78-e22c0688960a",
      "metadata": {
        "id": "c82ecb1a-b415-4c43-9a78-e22c0688960a"
      },
      "source": [
        "Używając tokenizera dla języka angielskiego do podzielenia zdania w dowolnym innym języku, widzimy, że otrzymujemy znacznie większą liczbę tokenów. Aby zobaczyć, jak tokenizator podzielił tekst, możemy użyć wywołania `covert_ids_to_tokens`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f97b1473-4902-4995-9883-1ec4c6efb498",
      "metadata": {
        "id": "f97b1473-4902-4995-9883-1ec4c6efb498"
      },
      "outputs": [],
      "source": [
        "print(\"|\".join(tokenizer.convert_ids_to_tokens(list(sentence1[0]))))\n",
        "print(\"|\".join(tokenizer.convert_ids_to_tokens(list(sentence2[0]))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7169458a-9707-4d17-a6b3-6bf00e452766",
      "metadata": {
        "id": "7169458a-9707-4d17-a6b3-6bf00e452766"
      },
      "source": [
        "Widzimy, że w przypadku języka angielskiego wszystkie słowa w zdaniu zostały przekonwertowane na pojedyncze tokeny. W przypadku zdania w dowolnym innym języku zawierającym szereg znaków diakrytycznych, sytuacja wygląda zupełnie inaczej - każdy znak został wyodrębniony w osobny pod-token. Fakt, że mamy do czynienia z pod-tokenami jest sygnalizowany przez dwa krzyżyki poprzedzające dany pod-token. Wskazują one, że ten pod-token musi zostać sklejony z poprzedzającym go tokenem w celu uzyskania poprawnego ciągu znaków).\n",
        "\n",
        "## Ćwiczenie\n",
        "\n",
        "Użyj tokenizera dla `xlm-roberta-large` do tokenizacji tych samych zdań. Jakie wnioski można wyciągnąć, patrząc na sposób tokenizacji przy użyciu różnych słowników?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b0231da-b2d5-4840-af4e-12d26c1036ea",
      "metadata": {
        "id": "0b0231da-b2d5-4840-af4e-12d26c1036ea"
      },
      "outputs": [],
      "source": [
        "# your code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49980786-5525-442b-9c9d-26b4e0a2cda4",
      "metadata": {
        "id": "49980786-5525-442b-9c9d-26b4e0a2cda4"
      },
      "source": [
        "W wyniku tokenizacji, oprócz słów/tokenów obecnych w oryginalnym tekście, w wynikach tokenizacji pojawiają się dodatkowe znaczniki [CLS] i [SEP] (lub inne znaczniki - w zależności od używanego słownika). Mają one specjalne znaczenie i mogą być wykorzystywane do wykonywania określonych funkcji związanych z analizą tekstu. Na przykład reprezentacja tokenu [CLS] jest używana w zadaniach klasyfikacji zdań. Z kolei token [SEP] służy do rozróżniania zdań w zadaniach wymagających dwóch zdań jako danych wejściowych (np. określania, jak bardzo zdania są do siebie podobne).\n",
        "\n",
        "##Modelowanie języka\n",
        "\n",
        "Modele wstępnie przetworzone w reżimie samonadzorowanego uczenia się (SSL) nie mają specjalnych możliwości rozwiązywania określonych zadań przetwarzania języka naturalnego, takich jak odpowiadanie na pytania lub klasyfikowanie tekstu (z wyjątkiem bardzo dużych modeli, takich jak na przykład GPT-3). Mogą być jednak wykorzystywane do określania prawdopodobieństwa występowania słów w tekście, a tym samym do testowania, jak dużą wiedzę posiada dany model w zakresie znajomości języka lub ogólnej wiedzy o świecie.\n",
        "\n",
        "Aby sprawdzić, jak model radzi sobie w tych zadaniach, możemy przeprowadzić wnioskowanie na danych wejściowych, w których niektóre słowa zostaną zastąpione specjalnymi symbolami maskującymi używanymi podczas wstępnego szkolenia modelu.\n",
        "\n",
        "\n",
        "Należy pamiętać, że różne modele mogą używać różnych sekwencji specjalnych podczas szkolenia wstępnego. Na przykład Bert używa sekwencji [MASK]. Możemy sprawdzić wygląd tokena maski lub jego identyfikator w [pliku konfiguracyjnym tokenizera](https://huggingface.co/bert-base-cased/raw/main/tokenizer.json) dystrybuowanym wraz z modelem.\n",
        "\n",
        "W pierwszym kroku spróbujemy uzupełnić brakujące słowo w zdaniu w języku angielskim."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3a217a7-a532-48c0-8cea-c8062019a01d",
      "metadata": {
        "id": "b3a217a7-a532-48c0-8cea-c8062019a01d"
      },
      "outputs": [],
      "source": [
        "sentence_en = tokenizer.encode(\n",
        "    \"The quick brown [MASK] jumps over the lazy dog.\", return_tensors=\"pt\"\n",
        ")\n",
        "print(\"|\".join(tokenizer.convert_ids_to_tokens(list(sentence_en[0]))))\n",
        "target = model(sentence_en)\n",
        "print(target.logits[0][4])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75c9fd9a-cb84-46ed-a8f7-1768e9ceaea5",
      "metadata": {
        "id": "75c9fd9a-cb84-46ed-a8f7-1768e9ceaea5"
      },
      "source": [
        "Ponieważ zdanie jest uzupełnione tagiem `[CLS]` po stocenizacji, zamaskowane słowo znajduje się na pozycji 4. Wywołanie `call target.logits[0][4]` pokazuje tensor z rozkładem prawdopodobieństwa poszczególnych słów, który został określony na podstawie parametrów modelu. Możemy wybrać słowa o najwyższym prawdopodobieństwie za pomocą wywołania `torch.topk`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c85c49a-38f5-4c47-a346-712d758ceb9d",
      "metadata": {
        "id": "9c85c49a-38f5-4c47-a346-712d758ceb9d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "top = torch.topk(target.logits[0][4], 5)\n",
        "top"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9f4ce04-e2b7-4370-a2c6-56836f39e037",
      "metadata": {
        "id": "e9f4ce04-e2b7-4370-a2c6-56836f39e037"
      },
      "source": [
        "Otrzymaliśmy dwa wektory - `wartości` zawierające składowe wektora wyjściowego sieci neuronowej (nieznormalizowane) oraz `indeksy` zawierające indeksy tych składowych. Na tej podstawie możemy wyświetlić wyrażenie, które według modelu jest najbardziej prawdopodobnym uzupełnieniem zamaskowanego wyrażenia:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac03276c-d635-4960-b29a-df539d63fae9",
      "metadata": {
        "id": "ac03276c-d635-4960-b29a-df539d63fae9"
      },
      "outputs": [],
      "source": [
        "words = tokenizer.convert_ids_to_tokens(top.indices)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.bar(words, top.values.detach().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d92b9e1-0b75-41ef-8b76-8c7b7857adfb",
      "metadata": {
        "id": "2d92b9e1-0b75-41ef-8b76-8c7b7857adfb"
      },
      "source": [
        "Zgodnie z oczekiwaniami, najbardziej prawdopodobnym zamiennikiem brakującego słowa jest pies. Drugie słowo ##ie może być nieco zaskakujące, ale po dodaniu do istniejącego tekstu otrzymujemy zdanie \"The quick brownie jumps over the lazy dog\", które również wydaje się sensowne (choć nieco zaskakujące).\n",
        "\n",
        "## Ćwiczenie\n",
        "\n",
        "Używając `xlm-roberta-model`, zaproponuj zdania z jednym brakującym słowem, weryfikując zdolność tego modelu do:\n",
        "\n",
        "uwzględnienia znaczenia w kontekście semantycznym,\n",
        "uwzględnienia relacji długodystansowych w tekście,\n",
        "reprezentowania wiedzy o świecie.\n",
        "Dla każdego problemu wymyśl 3 zdania testowe i wyświetl przewidywania dla 5 najbardziej prawdopodobnych słów.\n",
        "\n",
        "Postaraj się wymyślić przykłady z zamaskowanym elementem w różnych pozycjach w zdaniu.\n",
        "\n",
        "Możesz użyć kodu z funkcji `plot_words`, aby wyświetlić wyniki. Sprawdź również, jaki token maskujący jest używany w tym modelu i pamiętaj o załadowaniu `xlm-roberta-model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "917224d9-18e9-4541-a469-e5f223daf05f",
      "metadata": {
        "id": "917224d9-18e9-4541-a469-e5f223daf05f"
      },
      "outputs": [],
      "source": [
        "def plot_words(sentence, word_model, word_tokenizer, mask=\"[MASK]\"):\n",
        "    sentence = word_tokenizer.encode(sentence, return_tensors=\"pt\")\n",
        "    tokens = word_tokenizer.convert_ids_to_tokens(list(sentence[0]))\n",
        "    print(\"|\".join(tokens))\n",
        "    target = word_model(sentence)\n",
        "    top = torch.topk(target.logits[0][tokens.index(mask)], 5)\n",
        "    words = word_tokenizer.convert_ids_to_tokens(top.indices)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.bar(words, top.values.detach().numpy())\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# your code"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
