{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faac065c-ab39-45cd-aa8e-3a521dddb9ed",
   "metadata": {},
   "source": [
    "### Multiword Expressions\n",
    "\n",
    "\n",
    "Wydzielaliśmy do tej pory słowa w izolacji, choć wiemy, że występują one grupami. Jak stwierdzić, czy to, co obserwujemy w tekście to przypadkowa konstrukcja (nazwy rzeczy wymieniane jednym tchem, występujące obok siebie takie a nie inne słowa) czy może stałe połączenie wyrazowe, które możemy badać jako zjawisko (na wiele sposobów!) lub wykluczyć z korpusu, który poddajemy badaniom. \n",
    "\n",
    "Trwałe połączenia wyrazowe to frazy, które znamy jako przysłowia, powiedzonka, nazwy, złożenia. Więcej o nich i ich istocie można poczytać tu: https://en.wikipedia.org/wiki/Multiword_expression. Warto jednak zaznaczyć, że koncepcja jednostki wielowyrazowej ulega modyfikacjom w zależności od obranego paradygmatu badawczego! Dobrze jest przemyśleć sobie, co rozumiemy przez tę konstrukcję i konsekwentnie stosować się do tej wizji w naszych badaniach.\n",
    "\n",
    "Przedmiotem tego laboratorium jest koncept samego poszukiwania jednostek wielowyrazowych, ich opisu gramatycznego, zestawienia częstych z rzadkimi wystąpieniami w korpusie.\n",
    "\n",
    "W ramach wykrywania bigramów, będziemy kolejno:\n",
    "1. Pobierać i tokenizować korpus FIQA.\n",
    "2. Obliczać liczbę wystąpień złożeń dwóch elementów w tekście. Dla znanego przez nas zdania: The quick brown fox jumped over lazy dog rozłożenie takich dwuelementowych segmentów wygląda następująco:\n",
    "\"the quick\": 1\n",
    "\"quick brown\": 1\n",
    "\"brown fox\": 1\n",
    "...\n",
    "\"dog .\": 1\n",
    "3. Wyselekcjonujemy te segmenty, które tworzone są tylko przez słowa (ciekawość badacza: dlaczego teraz? dlaczego działamy w taki sposób?)\n",
    "4. Użyjemy miary Pointwise Mutual Informatio (PMI), by obliczyć siłę powiązań pomiędzy elementami tworzącymi segment. PMI (https://en.wikipedia.org/wiki/Pointwise_mutual_information) to tylko jeden ze sposobów na otrzymanie takiej informacji, inne opisano tu: https://onlinelibrary.wiley.com/doi/full/10.1111/lang.12225.\n",
    "5. Znajdziemy najczęstsze segmenty występujące w tekście. Określimy też próg przypadkowych połączeń wyrazów jako 5 wystąpień, nie będzie nas interesowało nic, co występuje 5 razy i rzadziej.\n",
    "6. Spojrzymy na cały problem z punktu widzenia składni, a więc oznaczymy segmenty, przypisując im interpretację morfologiczną, policzymy, które złożenia są najczęstsze (ciekawość badacza: jak najczęstrze kategorie morfologiczne mają się w porównaniu do naszych najczęstszych słów z wcześniejszych punktów?\n",
    "\n",
    "Zaczynamy od przygotowania środowiska."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c3ff21a-3cbf-47cc-b5c5-e49bce377ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "from datasets import load_dataset\n",
    "from spacy.lang.pl import Polish\n",
    "\n",
    "##!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b906457c",
   "metadata": {},
   "source": [
    "Użyte moduły:\n",
    "- `string` https://docs.python.org/3/library/string.html\n",
    "- `collections` https://docs.python.org/3/library/collections.html\n",
    "- `itertools` https://docs.python.org/3/library/itertools.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac9a34f-c0ab-4929-b669-9de31cb20ab3",
   "metadata": {},
   "source": [
    "## Zadanie 1\n",
    "\n",
    "Pobieramy korpus, tokenizujemy go, używając do tego znanych już narzędzi i zasobów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b02a10a6-e53c-4d04-a883-889a5ad7991e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset fiqa-pl (C:/Users/AGH/.cache/huggingface/datasets/clarin-knext___fiqa-pl/corpus/0.0.0/bada00640881ee3fd04c3b88df9edd435616d17e0a46faf05e63063858742140)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb93dbf55beb45c2a599ec0e5bb7ec3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['nie', 'mówię', ',', 'że', 'nie', 'podoba', 'mi', 'się', 'też', 'pomysł']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('clarin-knext/fiqa-pl', 'corpus')['corpus']['text']\n",
    "dataset = ' '.join(dataset)\n",
    "\n",
    "nlp = Polish()\n",
    "tokenizer = nlp.tokenizer\n",
    "\n",
    "tokens = tokenizer(dataset)\n",
    "tokens = list(map(lambda t: t.text.lower(), tokens))\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d08493-35b0-415e-9a8f-7530673f64d7",
   "metadata": {},
   "source": [
    "## Zadanie 2-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6f7b6b-afff-4050-90f6-ec5187378428",
   "metadata": {},
   "source": [
    "Obliczamy bigramy:\n",
    "(tu sprytne wyjaśnienie: https://www.exploredatabase.com/2020/04/bigram-probability-estimation-of-word-sequence-example.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72631447-0003-41f7-bf57-ec920503c6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. nie mówię: 276\n",
      "2. mówię ,: 429\n",
      "3. , że: 86094\n",
      "4. że nie: 5014\n",
      "5. nie podoba: 164\n",
      "6. podoba mi: 234\n",
      "7. mi się: 1398\n",
      "8. się też: 98\n",
      "9. też pomysł: 3\n",
      "10. pomysł szkolenia: 1\n",
      "11. szkolenia w: 7\n",
      "12. w miejscu: 210\n",
      "13. miejscu pracy: 69\n",
      "14. pracy ,: 1638\n",
      "15. , ale: 32549\n",
      "16. ale nie: 4242\n",
      "17. nie możesz: 2535\n",
      "18. możesz oczekiwać: 55\n",
      "19. oczekiwać ,: 246\n",
      "20. że firma: 517\n",
      "21. firma to: 28\n",
      "22. to zrobi: 86\n",
      "23. zrobi .: 59\n",
      "24. . szkolenie: 13\n",
      "25. szkolenie pracowników: 2\n",
      "26. pracowników to: 10\n",
      "27. to nie: 4412\n",
      "28. nie ich: 45\n",
      "29. ich praca: 27\n",
      "30. praca –: 3\n",
      "31. – oni: 3\n",
      "32. oni tworzą: 2\n",
      "33. tworzą oprogramowanie: 1\n",
      "34. oprogramowanie .: 33\n",
      "35. . być: 693\n",
      "36. być może: 2075\n",
      "37. może systemy: 1\n",
      "38. systemy edukacyjne: 1\n",
      "39. edukacyjne w: 2\n",
      "40. w stanach: 953\n",
      "41. stanach zjednoczonych: 935\n",
      "42. zjednoczonych (: 37\n",
      "43. ( lub: 2155\n",
      "44. lub ich: 76\n",
      "45. ich studenci: 1\n",
      "46. studenci ): 4\n",
      "47. ) powinny: 15\n",
      "48. powinny trochę: 2\n",
      "49. trochę martwić: 2\n"
     ]
    }
   ],
   "source": [
    "bigrams = [a + ' ' + b for a, b in zip(tokens, tokens[1:])]\n",
    "bigrams = list(filter(lambda x: len(x.split(' ')) == 2, bigrams))\n",
    "bigrams_counter = Counter(bigrams)\n",
    "\n",
    "for i, (key, val) in zip(range(1, 50), bigrams_counter.items()):\n",
    "    print(f'{i}. {key}: {val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c294487-52c3-4b98-8dae-42db9c8b88a3",
   "metadata": {},
   "source": [
    "I filtrujemy bigramy, aby zachować tylko takie, które zawierają wyrazy składające się wyłącznie z liter polskiego alfabetu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fc8a2c1-1229-4aba-b2a6-be071a811e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. nie mówię: 276\n",
      "2. że nie: 5014\n",
      "3. nie podoba: 164\n",
      "4. podoba mi: 234\n",
      "5. mi się: 1398\n",
      "6. się też: 98\n",
      "7. też pomysł: 3\n",
      "8. pomysł szkolenia: 1\n",
      "9. szkolenia w: 7\n",
      "10. w miejscu: 210\n",
      "11. miejscu pracy: 69\n",
      "12. ale nie: 4242\n",
      "13. nie możesz: 2535\n",
      "14. możesz oczekiwać: 55\n",
      "15. że firma: 517\n",
      "16. firma to: 28\n",
      "17. to zrobi: 86\n",
      "18. szkolenie pracowników: 2\n",
      "19. pracowników to: 10\n",
      "20. to nie: 4412\n",
      "21. nie ich: 45\n",
      "22. ich praca: 27\n",
      "23. oni tworzą: 2\n",
      "24. tworzą oprogramowanie: 1\n",
      "25. być może: 2075\n",
      "26. może systemy: 1\n",
      "27. systemy edukacyjne: 1\n",
      "28. edukacyjne w: 2\n",
      "29. w stanach: 953\n",
      "30. stanach zjednoczonych: 935\n",
      "31. lub ich: 76\n",
      "32. ich studenci: 1\n",
      "33. powinny trochę: 2\n",
      "34. trochę martwić: 2\n",
      "35. martwić się: 112\n",
      "36. się o: 1728\n",
      "37. o zdobycie: 10\n",
      "38. zdobycie umiejętności: 1\n",
      "39. umiejętności rynkowych: 8\n",
      "40. rynkowych w: 13\n",
      "41. w zamian: 465\n",
      "42. zamian za: 333\n",
      "43. za ich: 186\n",
      "44. ich ogromne: 2\n",
      "45. ogromne inwestycje: 4\n",
      "46. inwestycje w: 185\n",
      "47. w edukację: 21\n",
      "48. zamiast wychodzić: 3\n",
      "49. wychodzić z: 20\n"
     ]
    }
   ],
   "source": [
    "def discard_elements(iterable, filter_fn):\n",
    "    discard = []\n",
    "    \n",
    "    for element in iterable:\n",
    "        if filter_fn(element):\n",
    "            discard.append(element)\n",
    "            continue\n",
    "    \n",
    "    for d in discard:\n",
    "        del iterable[d]\n",
    "\n",
    "\n",
    "all_chars = string.ascii_letters + 'ęóąśłżźćńĘÓĄŚŁŻŹĆŃ '\n",
    "filter_not_letters = lambda element: any([c not in all_chars for c in element])\n",
    "\n",
    "discard_elements(bigrams_counter, filter_not_letters)\n",
    "\n",
    "for i, (key, val) in zip(range(1, 50), bigrams_counter.items()):\n",
    "    print(f'{i}. {key}: {val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d804a3-3722-40e9-8f50-e20ade5d76c7",
   "metadata": {},
   "source": [
    "## Zadanie 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d84bd48-a252-43fa-9c69-ab3461158162",
   "metadata": {},
   "source": [
    "Używamy PMI, żeby stwierdzić, jakie jest prawdopodobieństwo współwystępowania słów z bigramu w korpusie - PMI przybliża siłę powiązania słów. Im wyższy wynik, tym mocniej powiązane są słowa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c885a08e-d2b4-4c0b-9c4d-2dde5d53c92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-4.416926295451629, 'nie mówię'),\n",
       " (-6.757987787011004, 'że nie'),\n",
       " (-4.208622682465007, 'nie podoba'),\n",
       " (0.8842670497309287, 'podoba mi'),\n",
       " (-3.9519473098108984, 'mi się'),\n",
       " (-7.324130378143073, 'się też'),\n",
       " (-6.423896465035036, 'też pomysł'),\n",
       " (-3.1341487758968243, 'pomysł szkolenia'),\n",
       " (-7.28737177449006, 'szkolenia w'),\n",
       " (-5.474170594496768, 'w miejscu'),\n",
       " (-2.491980962135719, 'miejscu pracy'),\n",
       " (-5.8753647825451685, 'ale nie'),\n",
       " (-5.790793503182507, 'nie możesz'),\n",
       " (-2.9420546603597835, 'możesz oczekiwać'),\n",
       " (-5.648675281555828, 'że firma'),\n",
       " (-10.226940957378476, 'firma to'),\n",
       " (-4.735397496939331, 'to zrobi'),\n",
       " (-2.45197247245145, 'szkolenie pracowników'),\n",
       " (-10.204730973533314, 'pracowników to'),\n",
       " (-7.314117119818877, 'to nie'),\n",
       " (-10.930620257899763, 'nie ich'),\n",
       " (-5.040290601186146, 'ich praca'),\n",
       " (-2.654767456453104, 'oni tworzą'),\n",
       " (-2.220646926344994, 'tworzą oprogramowanie'),\n",
       " (-3.488812977099607, 'być może'),\n",
       " (-8.573072285148388, 'może systemy'),\n",
       " (0.23089461624169236, 'systemy edukacyjne'),\n",
       " (-7.889906904659166, 'edukacyjne w'),\n",
       " (-3.3397501418832634, 'w stanach'),\n",
       " (3.499120254353566, 'stanach zjednoczonych'),\n",
       " (-8.189630787875357, 'lub ich'),\n",
       " (-6.250857587125804, 'ich studenci'),\n",
       " (-6.669903007990557, 'powinny trochę'),\n",
       " (-5.4031550993577, 'trochę martwić'),\n",
       " (-4.091153206165459, 'martwić się'),\n",
       " (-6.443178003004493, 'się o'),\n",
       " (-4.9785776314708885, 'o zdobycie'),\n",
       " (-3.0776978420841905, 'zdobycie umiejętności'),\n",
       " (-1.0121095004566139, 'umiejętności rynkowych'),\n",
       " (-7.73308889185833, 'rynkowych w'),\n",
       " (-3.019621061817307, 'w zamian'),\n",
       " (-1.0613257193521932, 'zamian za'),\n",
       " (-6.858814419751923, 'za ich'),\n",
       " (-7.717175591348341, 'ich ogromne'),\n",
       " (-3.539307307398446, 'ogromne inwestycje'),\n",
       " (-6.128727367782991, 'inwestycje w'),\n",
       " (-6.779853359194252, 'w edukację'),\n",
       " (-1.2648088045344654, 'zamiast wychodzić'),\n",
       " (-3.322177237088152, 'wychodzić z'),\n",
       " (-3.2028783087158077, 'z tysiącami')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_counter = Counter(tokens)\n",
    "\n",
    "\n",
    "def pmi(bigram):\n",
    "    x, y = bigram.split(' ')\n",
    "    p_x = tokens_counter[x] / len(tokens_counter)\n",
    "    p_y = tokens_counter[y] / len(tokens_counter)\n",
    "    p_xy = bigrams_counter[bigram] / len(bigrams_counter)\n",
    "\n",
    "    if p_x * p_y == 0 or p_xy == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.log2(p_xy / (p_x * p_y))\n",
    "\n",
    "\n",
    "bigrams_pmi = [(pmi(bigram), bigram) for bigram in bigrams_counter]\n",
    "bigrams_pmi[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39095ead-daeb-4bf5-b8c1-2e03dfc6d1da",
   "metadata": {},
   "source": [
    "## Zadanie 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db24d13d-25d1-4e1a-b1c9-bde9050b8c32",
   "metadata": {},
   "source": [
    "Bezpośrednie zastosowanie PMI na korpusie zwraca dość charakterystyczne bigramy, które wyglądają, jakby występowały w korpusie bardzo rzadko (zarówno bigramy, jak i tworzące je tokeny) i dlatego znalazły się na początku rankingu (przez bardzo małe prawdopodobieństwa $p_x, p_y$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5479af8-f572-43b8-9084-4d97acccaab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(14.412745683990813, 'żelem silikonowym'),\n",
       " (14.412745683990813, 'żarówkom żarowym'),\n",
       " (14.412745683990813, 'świnkami morskimi'),\n",
       " (14.412745683990813, 'światłami wystawowymi'),\n",
       " (14.412745683990813, 'śródrocznym skróconym'),\n",
       " (14.412745683990813, 'śrubokręta torx'),\n",
       " (14.412745683990813, 'środkowi mającemu'),\n",
       " (14.412745683990813, 'śmietanki owsianej'),\n",
       " (14.412745683990813, 'śmiertelnemu konfliktowi'),\n",
       " (14.412745683990813, 'śmierdzącego spoconego'),\n",
       " (14.412745683990813, 'śmiałość insynuującą'),\n",
       " (14.412745683990813, 'ślinka cieknie'),\n",
       " (14.412745683990813, 'ściśliwych piankowych'),\n",
       " (14.412745683990813, 'ściernych gąbek'),\n",
       " (14.412745683990813, 'łąki kośne'),\n",
       " (14.412745683990813, 'łyżwiarstwo figurowe'),\n",
       " (14.412745683990813, 'łukiem diamentowym'),\n",
       " (14.412745683990813, 'łopatką wieprzową'),\n",
       " (14.412745683990813, 'łaźnią olejową'),\n",
       " (14.412745683990813, 'łazikach marsjańskich'),\n",
       " (14.412745683990813, 'łatwopalnymi paczkami'),\n",
       " (14.412745683990813, 'łajdakami wykorzystującymi'),\n",
       " (14.412745683990813, 'łagodnymi mdłościami'),\n",
       " (14.412745683990813, 'ładowaczem dokowym'),\n",
       " (14.412745683990813, 'ćwiczyła younique'),\n",
       " (14.412745683990813, 'ówdzie porozrzucanych'),\n",
       " (14.412745683990813, 'złączkami wodociągowymi'),\n",
       " (14.412745683990813, 'ząbkową wróżkę'),\n",
       " (14.412745683990813, 'zygmunt freud'),\n",
       " (14.412745683990813, 'związaniu zaślubin'),\n",
       " (14.412745683990813, 'zwierzynę łowną'),\n",
       " (14.412745683990813, 'zwarłeś dcth'),\n",
       " (14.412745683990813, 'zwaartekracht pijnlijk'),\n",
       " (14.412745683990813, 'zure bikotearekin'),\n",
       " (14.412745683990813, 'zur tagespflege'),\n",
       " (14.412745683990813, 'zum einsatz'),\n",
       " (14.412745683990813, 'zuboża wierzchnią'),\n",
       " (14.412745683990813, 'zszywacz pneumatyczny'),\n",
       " (14.412745683990813, 'zrzędliwy starzec'),\n",
       " (14.412745683990813, 'zrozumiełeś powojenną'),\n",
       " (14.412745683990813, 'zowel anker'),\n",
       " (14.412745683990813, 'zorgeloze spevener'),\n",
       " (14.412745683990813, 'zonas rurales'),\n",
       " (14.412745683990813, 'zoho invoice'),\n",
       " (14.412745683990813, 'znakowane wybuchającym'),\n",
       " (14.412745683990813, 'znaczenierosemarie fike'),\n",
       " (14.412745683990813, 'zjebali mroźny'),\n",
       " (14.412745683990813, 'ziomki zbombardowały'),\n",
       " (14.412745683990813, 'ziejącą paszczę'),\n",
       " (14.412745683990813, 'zestrzeliły prosyryjskiego')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(bigrams_pmi, reverse=True)[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931ed989-7e2a-49f6-b6fe-50734af0ae70",
   "metadata": {},
   "source": [
    "Odfiltrowanie najrzadszych bigramów pozwala uzyskać bardziej rozsądne wyniki - większość naszych aktualnych wyników to charakterystyczne określenia składające się z dwóch słów (np. \"klęska żywiołowa\", \"książeczka czekowa\") lub nazwy własne (np. \"Stucco Veneziano\", \"Bert Hellinger\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f369f21a-fb74-4001-b5fc-c8d389a9c380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15.656543516720237, 'stucco veneziano'),\n",
       " (15.656543516720237, 'królicza nora'),\n",
       " (15.656543516720237, 'klęska żywiołowa'),\n",
       " (15.656543516720237, 'bert hellinger'),\n",
       " (15.393509110886443, 'seair exim'),\n",
       " (15.393509110886443, 'sameer thakar'),\n",
       " (15.393509110886443, 'książeczka czekowa'),\n",
       " (15.393509110886443, 'gone fishin'),\n",
       " (15.393509110886443, 'electro plating'),\n",
       " (15.393509110886443, 'deming electro'),\n",
       " (15.171116689549995, 'wózkiem widłowym'),\n",
       " (15.171116689549995, 'stwardnienia rozsianego'),\n",
       " (15.171116689549995, 'stawu biodrowego'),\n",
       " (15.171116689549995, 'napędów taśmowych'),\n",
       " (15.171116689549995, 'kuala lumpur'),\n",
       " (15.171116689549995, 'króliczej nory'),\n",
       " (15.171116689549995, 'klatkę piersiową'),\n",
       " (15.171116689549995, 'billem gatesem'),\n",
       " (15.171116689549995, 'autot ldr'),\n",
       " (14.978471611607599, 'psychoterapeuta bert'),\n",
       " (14.978471611607599, 'obciążeniami zwrotnymi'),\n",
       " (14.978471611607599, 'kushagra nayan'),\n",
       " (14.978471611607599, 'konter pulsa'),\n",
       " (14.978471611607599, 'gałki oczne'),\n",
       " (14.948724268213548, 'metalami szlachetnymi'),\n",
       " (14.948724268213548, 'limo mia'),\n",
       " (14.808546610165287, 'przybory toaletowe'),\n",
       " (14.808546610165287, 'pertho engineers'),\n",
       " (14.808546610165287, 'miecz obosieczny'),\n",
       " (14.808546610165287, 'kozła ofiarnego'),\n",
       " (14.808546610165287, 'grysy kwarcowe'),\n",
       " (14.808546610165287, 'biegłego rewidenta'),\n",
       " (14.808546610165287, 'agenzija sedqa'),\n",
       " (14.756079190271151, 'warrenem buffettem'),\n",
       " (14.756079190271151, 'honorariów adwokackich'),\n",
       " (14.715437205773807, 'wytwórnie płytowe'),\n",
       " (14.715437205773807, 'olive garden'),\n",
       " (14.715437205773807, 'exim solution'),\n",
       " (14.685689862379753, 'śliskim zboczu'),\n",
       " (14.685689862379753, 'swiss bullion'),\n",
       " (14.685689862379753, 'sina bugeja'),\n",
       " (14.656543516720237, 'tkanki tłuszczowej'),\n",
       " (14.656543516720237, 'rolls royce'),\n",
       " (14.656543516720237, 'molestowania seksualnego'),\n",
       " (14.656543516720237, 'lightning eliminators'),\n",
       " (14.656543516720237, 'liberalized remittance'),\n",
       " (14.656543516720237, 'kozłem ofiarnym'),\n",
       " (14.656543516720237, 'dojną krową'),\n",
       " (14.656543516720237, 'arkuszami kalkulacyjnymi'),\n",
       " (14.545512204331493, 'setha klarmana')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_rare = lambda element: bigrams_counter[element] < 5\n",
    "discard_elements(bigrams_counter, filter_rare)\n",
    "\n",
    "bigrams_pmi = [(pmi(bigram), bigram) for bigram in bigrams_counter]\n",
    "top_bigrams_pmi = sorted(bigrams_pmi, reverse=True)[:50]\n",
    "top_bigrams_pmi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd20f5c-b0e7-4840-bb22-231d60a10298",
   "metadata": {},
   "source": [
    "## Zadanie 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df6bddf",
   "metadata": {},
   "source": [
    "Na wyniki patrzymy teraz z innej perspektywy - chcemy zobaczyć, jak najczęstsze segmenty rozkładają się w ujęciu gramatycznym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bd1165f-7249-430f-824a-3afb68cf612d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset fiqa-pl (C:/Users/AGH/.cache/huggingface/datasets/clarin-knext___fiqa-pl/corpus/0.0.0/bada00640881ee3fd04c3b88df9edd435616d17e0a46faf05e63063858742140)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a32e2c5ddf46d7aba30d2d38fca948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. nie:qub\n",
      "2. mówić:fin\n",
      "3. ,:interp\n",
      "4. że:comp\n",
      "5. nie:qub\n",
      "6. podobać:fin\n",
      "7. ja:ppron12\n",
      "8. się:qub\n",
      "9. też:qub\n",
      "10. pomysł:subst\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('pl_core_news_sm')\n",
    "\n",
    "dataset = load_dataset('clarin-knext/fiqa-pl', 'corpus')['corpus']['text']\n",
    "dataset = map(nlp, dataset)\n",
    "dataset = list(map(lambda word: f'{word.lemma_}:{word.tag_}'.lower(), chain.from_iterable(dataset)))\n",
    "\n",
    "for i, word in enumerate(dataset[:10], 1):\n",
    "    print(f'{i}. {word}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a8c200-2644-4993-88c8-1686f692f59e",
   "metadata": {},
   "source": [
    "Ranking top-10 wartości PMI obliczony na korpusie po lematyzacji oraz tagowaniu daje bardzo podobne wyniiki - pojawiają się w nim te same lub należące do tych samych kategorii bigramy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "794c1ded-bb67-475f-a9cf-6e5baf10c673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. nie:qub mówić:fin: 529\n",
      "2. że:comp nie:qub: 4901\n",
      "3. nie:qub podobać:fin: 155\n",
      "4. podobać:fin ja:ppron12: 246\n",
      "5. ja:ppron12 się:qub: 1407\n",
      "6. się:qub też:qub: 97\n",
      "7. szkolenie:ger w:prep: 5\n",
      "8. w:prep miejsce:subst: 313\n",
      "9. miejsce:subst praca:subst: 1080\n",
      "10. ale:conj nie:qub: 4228\n",
      "11. nie:qub móc:fin: 6580\n",
      "12. móc:fin oczekiwać:inf: 88\n",
      "13. że:comp firma:subst: 735\n",
      "14. firma:subst to:pred: 92\n",
      "15. to:pred zrobić:subst: 150\n",
      "16. pracownik:subst to:pred: 17\n",
      "17. to:pred nie:qub: 2701\n",
      "18. nie:qub on:ppron3: 89\n",
      "19. on:ppron3 praca:subst: 145\n",
      "20. on:ppron3 tworzyć:fin: 12\n",
      "21. być:inf może:fin: 1342\n",
      "22. system:subst edukacyjny:adj: 9\n",
      "23. edukacyjny:adj w:prep: 9\n",
      "24. w:prep stany:subst: 921\n",
      "25. stany:subst zjednoczone:adj: 1951\n",
      "26. lub:conj on:ppron3: 203\n",
      "27. on:ppron3 student:subst: 5\n",
      "28. powinien:winien trochę:adv: 6\n",
      "29. martwić:inf się:qub: 112\n",
      "30. się:qub o:prep: 1727\n",
      "31. umiejętność:subst rynkowy:adj: 6\n",
      "32. rynkowy:adj w:prep: 91\n",
      "33. w:prep zamian:burk: 422\n",
      "34. zamian:burk za:prep: 330\n",
      "35. za:prep on:ppron3: 745\n",
      "36. on:ppron3 ogromny:adj: 32\n",
      "37. ogromny:adj inwestycja:subst: 11\n",
      "38. inwestycja:subst w:prep: 796\n",
      "39. w:prep edukacja:subst: 39\n",
      "40. wychodzić:inf z:prep: 21\n",
      "41. z:prep tysiąc:subst: 25\n",
      "42. student:subst i:conj: 30\n",
      "43. i:conj narzekać:subst: 6\n",
      "44. nie:qub być:fin: 18307\n",
      "45. być:fin do:prep: 490\n",
      "46. do:prep nic:subst: 85\n",
      "47. tak:adv więc:conj: 2066\n",
      "48. więc:conj nic:subst: 23\n",
      "49. nic:subst nie:qub: 977\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(15.89580735073129, 'emiratów:subst arabskich:subst'),\n",
       " (15.89580735073129, 'bert:subst hellinger:subst'),\n",
       " (15.632772944897496, 'seair:subst exim:subst'),\n",
       " (15.632772944897496, 'sameer:subst thakar:subst'),\n",
       " (15.632772944897496, 'gone:subst fishin:subst'),\n",
       " (15.632772944897496, 'electro:subst plating:subst'),\n",
       " (15.632772944897496, 'deming:xxx electro:subst'),\n",
       " (15.410380523561049, 'autot:subst ldr:subst'),\n",
       " (15.217735445618652, 'nawiasach:subst kwadratowych:adj'),\n",
       " (15.217735445618652, 'kushagra:subst nayan:subst')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = [a + ' ' + b for a, b in zip(dataset, dataset[1:])]\n",
    "bigrams = list(filter(lambda x: len(x.split(' ')) == 2, bigrams))\n",
    "bigrams_counter = Counter(bigrams)\n",
    "tokens_counter = Counter(dataset)\n",
    "\n",
    "def filter_not_letters(element):\n",
    "    a, b = element.split(' ')\n",
    "    a, b = a.split(':')[0], b.split(':')[0]\n",
    "    return any([c not in all_chars for c in a + b])\n",
    "\n",
    "discard_elements(bigrams_counter, filter_not_letters)\n",
    "discard_elements(bigrams_counter, filter_rare)\n",
    "\n",
    "for i, (key, val) in zip(range(1, 50), bigrams_counter.items()):\n",
    "    print(f'{i}. {key}: {val}')\n",
    "\n",
    "print()\n",
    "\n",
    "bigrams_pmi = [(pmi(bigram), bigram) for bigram in bigrams_counter]\n",
    "top_bigrams_pmi_tag_lem = sorted(bigrams_pmi, reverse=True)[:10]\n",
    "top_bigrams_pmi_tag_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f8df42a-9e75-46ac-93ec-45ab8183b30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ('qub', 'fin')\n",
      "2. ('comp', 'qub')\n",
      "3. ('fin', 'ppron12')\n",
      "4. ('ppron12', 'qub')\n",
      "5. ('qub', 'qub')\n",
      "6. ('ger', 'prep')\n",
      "7. ('prep', 'subst')\n",
      "8. ('subst', 'subst')\n",
      "9. ('conj', 'qub')\n",
      "10. ('fin', 'inf')\n"
     ]
    }
   ],
   "source": [
    "groups = defaultdict(list)\n",
    "\n",
    "for bigram, count in bigrams_counter.items():\n",
    "    a, b = bigram.split(' ')\n",
    "    a, b = a.split(':')[1], b.split(':')[1]\n",
    "    groups[(a, b)].append((count, bigram))\n",
    "\n",
    "for i, key in zip(range(1, 11), groups):\n",
    "    print(f'{i}. {key}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b388d-08c4-44ab-82c7-90e31d7276ab",
   "metadata": {},
   "source": [
    "## Podsumowanie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a014bd6-7677-4413-b1ef-c03ce133acf6",
   "metadata": {},
   "source": [
    "Kategorie najczęściej spotykanych bigramów są dość intuicyjne, jeśli chodzi o język naturalny, wyniki są zgodne z regułami łączenia słów w języku polskim:\n",
    "\n",
    "- przyimek + rzeczownik (np. \"na przykład\"),\n",
    "- przymiotnik + rzeczownik (np. \"druga strona\"),\n",
    "- rzeczownik + przyimek (np. \"podatek od\"),\n",
    "- rzeczownik + rzeczownik (np. \"miejsce pracy\"),\n",
    "- przyimek + przymiotnik (np. \"w którym\"),\n",
    "- rzeczownik + przymiotnik (np. \"karta kredytowa\"),\n",
    "- rzeczownik + czasownik (np. \"to jest\"),\n",
    "- przeczenie + czasownik (np. \"nie jest\"),\n",
    "- czasownik + rzeczownik (np. \"oznacza to\"),\n",
    "- rzeczownik + spójnik (np. \"pieniądz i\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4db0056-59ba-4f10-8b36-ad8da0083073",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_count = [(sum(map(lambda g: g[0], val)), key) for key, val in groups.items()]\n",
    "groups_count = sorted(groups_count, reverse=True)\n",
    "groups_count[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea77412-2a2a-42c2-bad4-152554df7455",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, group in groups_count[:10]:\n",
    "    print('-' * 50)\n",
    "    print(group)\n",
    "    \n",
    "    for i, v in enumerate(sorted(groups[group], reverse=True)[:5], 1):\n",
    "        print(f'{i}. [{v[0]}] {v[1]}')\n",
    "        \n",
    "print('-' * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
