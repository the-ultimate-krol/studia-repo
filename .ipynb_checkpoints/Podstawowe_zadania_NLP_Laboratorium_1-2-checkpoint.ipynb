{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRvCuHz36wh9"
   },
   "source": [
    "# Analiza danych tekstowych -- wstƒôp\n",
    "\n",
    "## ≈örodowisko analityczne\n",
    "\n",
    "Przed Wami kr√≥tkie wprowadzenie do podstwowych zada≈Ñ z obszaru NLP. Wykonamy je na dostƒôpnych danych, ma≈Ço wymagajƒÖcym pipelinie i dla jƒôzyka angielskiego. Ale uwaga: analogiczne zadania bƒôdziecie robiƒá dla jƒôzyka polskiego (praca samodzielna, analogiczna do prezentowanej dla jƒôzyka angielskiego).\n",
    "\n",
    "Bƒôdziemy u≈ºywaƒá pakietu `spacy`, kt√≥ry sprawdza siƒô przy analizie tekstu, `scikit-learn` do oblicze≈Ñ i `matplotlip`, kt√≥ry pomo≈ºe zaprezentowaƒá dane na wykresach. Jako dane anglojƒôzyczne we≈∫miemy sobie `en_core_web_sm` --> model od SpaCy trenowany na tekstach news√≥w z jƒôzyka angielskiego. Pierwsze zadanie dla Was: Jaki pakiet danych odpowiada za dane z jƒôzyka polskiego?\n",
    "\n",
    "Modu≈Ç `datasets` odpowiada za ≈Çatwe ≈Çadowanie danych. Dane pochodzƒÖ z [HuggingFace](https://huggingface.co/docs/datasets/v1.8.0/loading_datasets.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "b6vQYVbzRT2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.7.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.13.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.25.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (67.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.10.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (10.0.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]>=2021.11.1->datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2022.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 640.0 kB/s eta 0:00:20\n",
      "     ---------------------------------------- 0.1/12.8 MB 1.3 MB/s eta 0:00:10\n",
      "     ---------------------------------------- 0.1/12.8 MB 1.4 MB/s eta 0:00:09\n",
      "      --------------------------------------- 0.3/12.8 MB 1.6 MB/s eta 0:00:08\n",
      "      --------------------------------------- 0.3/12.8 MB 1.6 MB/s eta 0:00:08\n",
      "     - -------------------------------------- 0.4/12.8 MB 1.6 MB/s eta 0:00:08\n",
      "     - -------------------------------------- 0.5/12.8 MB 1.6 MB/s eta 0:00:08\n",
      "     - -------------------------------------- 0.6/12.8 MB 1.6 MB/s eta 0:00:08\n",
      "     -- ------------------------------------- 0.7/12.8 MB 1.8 MB/s eta 0:00:07\n",
      "     -- ------------------------------------- 0.9/12.8 MB 2.1 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.0/12.8 MB 2.1 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.0/12.8 MB 2.0 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.1/12.8 MB 2.0 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.1/12.8 MB 2.0 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.3/12.8 MB 2.0 MB/s eta 0:00:06\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 2.1 MB/s eta 0:00:06\n",
      "     ---- ----------------------------------- 1.5/12.8 MB 2.1 MB/s eta 0:00:06\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 2.1 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 1.6/12.8 MB 2.2 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 2.2 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 2.1 MB/s eta 0:00:06\n",
      "     ------ --------------------------------- 1.9/12.8 MB 2.2 MB/s eta 0:00:05\n",
      "     ------ --------------------------------- 1.9/12.8 MB 2.2 MB/s eta 0:00:05\n",
      "     ------ --------------------------------- 2.0/12.8 MB 2.0 MB/s eta 0:00:06\n",
      "     ------ --------------------------------- 2.2/12.8 MB 2.1 MB/s eta 0:00:05\n",
      "     ------- -------------------------------- 2.4/12.8 MB 2.1 MB/s eta 0:00:05\n",
      "     ------- -------------------------------- 2.5/12.8 MB 2.2 MB/s eta 0:00:05\n",
      "     -------- ------------------------------- 2.7/12.8 MB 2.3 MB/s eta 0:00:05\n",
      "     -------- ------------------------------- 2.7/12.8 MB 2.3 MB/s eta 0:00:05\n",
      "     -------- ------------------------------- 2.8/12.8 MB 2.3 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 3.0/12.8 MB 2.3 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 3.1/12.8 MB 2.3 MB/s eta 0:00:05\n",
      "     ---------- ----------------------------- 3.2/12.8 MB 2.3 MB/s eta 0:00:05\n",
      "     ---------- ----------------------------- 3.2/12.8 MB 2.2 MB/s eta 0:00:05\n",
      "     ---------- ----------------------------- 3.3/12.8 MB 2.2 MB/s eta 0:00:05\n",
      "     ---------- ----------------------------- 3.3/12.8 MB 2.2 MB/s eta 0:00:05\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 2.2 MB/s eta 0:00:05\n",
      "     ----------- ---------------------------- 3.6/12.8 MB 2.2 MB/s eta 0:00:05\n",
      "     ----------- ---------------------------- 3.6/12.8 MB 2.1 MB/s eta 0:00:05\n",
      "     ----------- ---------------------------- 3.6/12.8 MB 2.1 MB/s eta 0:00:05\n",
      "     ----------- ---------------------------- 3.8/12.8 MB 2.1 MB/s eta 0:00:05\n",
      "     ------------ --------------------------- 3.9/12.8 MB 2.1 MB/s eta 0:00:05\n",
      "     ------------ --------------------------- 4.0/12.8 MB 2.1 MB/s eta 0:00:05\n",
      "     ------------ --------------------------- 4.0/12.8 MB 2.1 MB/s eta 0:00:05\n",
      "     ------------ --------------------------- 4.1/12.8 MB 2.1 MB/s eta 0:00:05\n",
      "     ------------ --------------------------- 4.1/12.8 MB 2.1 MB/s eta 0:00:05\n",
      "     ------------- -------------------------- 4.2/12.8 MB 2.1 MB/s eta 0:00:05\n",
      "     ------------- -------------------------- 4.3/12.8 MB 2.1 MB/s eta 0:00:05\n",
      "     ------------- -------------------------- 4.3/12.8 MB 2.0 MB/s eta 0:00:05\n",
      "     ------------- -------------------------- 4.3/12.8 MB 2.0 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 4.5/12.8 MB 2.0 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 4.7/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     --------------- ------------------------ 4.9/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "     --------------- ------------------------ 5.1/12.8 MB 2.2 MB/s eta 0:00:04\n",
      "     ---------------- ----------------------- 5.3/12.8 MB 2.2 MB/s eta 0:00:04\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 2.2 MB/s eta 0:00:04\n",
      "     ----------------- ---------------------- 5.7/12.8 MB 2.3 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 5.8/12.8 MB 2.3 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 6.0/12.8 MB 2.3 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 6.3/12.8 MB 2.4 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 6.5/12.8 MB 2.4 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 6.5/12.8 MB 2.4 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 6.8/12.8 MB 2.4 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 6.9/12.8 MB 2.5 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 6.9/12.8 MB 2.5 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 7.0/12.8 MB 2.4 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 2.4 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.2/12.8 MB 2.4 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 7.5/12.8 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.7/12.8 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.7/12.8 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.8/12.8 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.8/12.8 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.8/12.8 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.8/12.8 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.8/12.8 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.8/12.8 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.8/12.8 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.8/12.8 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.8/12.8 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.8/12.8 MB 2.2 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.8/12.8 MB 2.2 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.8/12.8 MB 2.1 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.8/12.8 MB 2.1 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.8/12.8 MB 2.1 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.9/12.8 MB 2.1 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.9/12.8 MB 2.1 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.9/12.8 MB 2.1 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 8.0/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 8.1/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 8.1/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 8.2/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 8.3/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 8.3/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 8.3/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 8.3/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 8.5/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 8.6/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 8.8/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 9.0/12.8 MB 2.0 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.1/12.8 MB 2.0 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.3/12.8 MB 2.0 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.5/12.8 MB 2.1 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.7/12.8 MB 2.1 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 9.9/12.8 MB 2.1 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 10.1/12.8 MB 2.1 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 10.3/12.8 MB 2.2 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 10.4/12.8 MB 2.2 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 10.8/12.8 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.1/12.8 MB 2.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.2/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.4/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.6/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.9/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.0/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.2/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 2.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 2.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.5/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.5/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from en-core-web-sm==3.6.0) (3.6.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.25.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\agh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy scikit-learn matplotlib datasets\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-4JZPM9X5xI"
   },
   "outputs": [],
   "source": [
    "# Miejsce na Tw√≥j kod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sPaOFZMRSku"
   },
   "source": [
    "## Tokenizacja\n",
    "\n",
    "Pierwsze zadanie polega na stokenizowaniu tekstu, co oznacza ni mniej ni wiƒôcej jak podzia≈Ç tekstu na najmniejsze znaczƒÖce elementy, kt√≥re wsp√≥lnie tworzƒÖ wyrazy tekstowe i tekst. Tokenizacja dla ka≈ºdego jƒôzyka jest inna, co mo≈ºecie zaobserwowaƒá zestawiajƒÖc dane dla jƒôzyka angielskiego z danymi dla jƒôzyka polskiego i por√≥wnujƒÖc wyniki uzyskane na tekstach.\n",
    "\n",
    "Tokenizacja przydaje siƒô w ≈ºyciu. PracujƒÖc na tokenach, pracujemy na uniwersalnych danych i por√≥wnujemy dane w spos√≥b niestronniczy.\n",
    "\n",
    "Por√≥wnajcie zdania:\n",
    "\"I'd like you to have some fun working with all those excercises like mice have fun with every piece of chees. It's all we need to have some fun in life.\"\n",
    "\"Chcia≈Çabym, ≈ºeby≈õcie mieli tyle zabawy z ƒáwiczeniami, ile ubawu majƒÖ myszy z ka≈ºdym jednym kawa≈Çeczkiem sera. Wszystko, czego potrzebujemy w ≈ºyƒáku, to zabawa.\"\n",
    "\n",
    "\n",
    "Wyja≈õnienie:\n",
    "Dowiadujemy siƒô tu jak tokenizowaƒá tekst, a za chwilƒô dowiemy siƒô, ≈ºe te tokeny mogƒÖ byƒá przetwarzane w wektory, na podstawie kt√≥rych mo≈ºemy przewidywaƒá kolejne wyrazy tekstowe. Je≈õli jednak wynik tokenizacji nie ma byƒá u≈ºyty bezpo≈õrednio jako dane wej≈õciowe sieci neuronowej, mo≈ºemy u≈ºyƒá przyjaznej tokenizacji z pakietu `spacy`.\n",
    "\n",
    "Zainicjujmy wiƒôc pakiet `spacy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vZr9Oy8P7SHE"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miejsce na Tw√≥j kod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfcttssX8Q16"
   },
   "source": [
    "Najpierw stokenizujmy tekst dla jƒôzyka angielskiego:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Y4a1_8Tf8Vwc",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"'d\",\n",
       " 'like',\n",
       " 'you',\n",
       " 'to',\n",
       " 'have',\n",
       " 'some',\n",
       " 'fun',\n",
       " 'working',\n",
       " 'with',\n",
       " 'all',\n",
       " 'those',\n",
       " 'excercises',\n",
       " 'like',\n",
       " 'mice',\n",
       " 'have',\n",
       " 'fun',\n",
       " 'with',\n",
       " 'every',\n",
       " 'piece',\n",
       " 'of',\n",
       " 'chees',\n",
       " '.',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'all',\n",
       " 'we',\n",
       " 'need',\n",
       " 'to',\n",
       " 'have',\n",
       " 'some',\n",
       " 'fun',\n",
       " 'in',\n",
       " 'life',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"I'd like you to have some fun working with all those excercises like mice have fun with every piece of chees. It's all we need to have some fun in life.\"\"\"\n",
    "\n",
    "tokens_en = nlp_en(text)\n",
    "[token.text for token in tokens_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miejsce na Tw√≥j kod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRB7_tIjzujG"
   },
   "source": [
    "A potem dla polskiego:\n",
    "(pamiƒôtajmy o innej nazwie zmiennych!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y06DuFMVDmWO"
   },
   "source": [
    "A teraz podzielmy teksty na zdania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Iq9wbNqGDoox",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'd like you to have some fun working with all those excercises like mice have fun with every piece of chees.\",\n",
       " \"It's all we need to have some fun in life.\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sentence.text for sentence in tokens_en.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miejsce na Tw√≥j kod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNlfmjT60NLh"
   },
   "source": [
    "### ‚≠ê Zadanie sprawdzajƒÖce umiejƒôtno≈õci‚≠ê\n",
    "\n",
    "Stw√≥rz zdanie, kt√≥re:\n",
    "- zawiera skr√≥ty (USA, U.S., i.e., ww.)\n",
    "- zawiera nazwy (McDonald's, Kelly's)\n",
    "- zawiera czasowniki w formach warunkowych, przypuszczajƒÖcych (I would like, chcia≈Çabym).\n",
    "\n",
    "```\n",
    "np. We have been to U.K. before we got to the very special country, i.e. Poland.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hdKGB9UB-N3Z"
   },
   "outputs": [],
   "source": [
    "# Miejsce na Tw√≥j kod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z39blSV0-sof"
   },
   "source": [
    "## Wykrywanie kategorii morfologicznych\n",
    "\n",
    "`spacy` mo≈ºna u≈ºywaƒá te≈º do analizy kategorii morfologicznych (morfosk≈Çadniowych, zwanych te≈º *czƒô≈õciami mowy*, kt√≥re -- o zgrozo -- odmieniajƒÖ siƒô przez przypadki, osoby i stopnie). Tagowanie morfosk≈Çadniowe, inaczej Part-of-Speech Tagging (POS Tagging) przydaje siƒô w bardziej zaawansowanych zadaniach, mo≈ºe te≈º pozwoliƒá na wnikliwy wglƒÖd w w≈Ça≈õciwo≈õci tekstu.\n",
    "\n",
    "W tym zadaniu mo≈ºemy u≈ºyƒá token√≥w (`tokens_en`) z poprzednich ƒáwicze≈Ñ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AqCgDjRt-zMG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRON'),\n",
       " (\"'d\", 'AUX'),\n",
       " ('like', 'VERB'),\n",
       " ('you', 'PRON'),\n",
       " ('to', 'PART'),\n",
       " ('have', 'VERB'),\n",
       " ('some', 'DET'),\n",
       " ('fun', 'NOUN'),\n",
       " ('working', 'VERB'),\n",
       " ('with', 'ADP'),\n",
       " ('all', 'DET'),\n",
       " ('those', 'DET'),\n",
       " ('excercises', 'NOUN'),\n",
       " ('like', 'SCONJ'),\n",
       " ('mice', 'NOUN'),\n",
       " ('have', 'VERB'),\n",
       " ('fun', 'NOUN'),\n",
       " ('with', 'ADP'),\n",
       " ('every', 'DET'),\n",
       " ('piece', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('chees', 'NOUN'),\n",
       " ('.', 'PUNCT'),\n",
       " ('It', 'PRON'),\n",
       " (\"'s\", 'AUX'),\n",
       " ('all', 'PRON'),\n",
       " ('we', 'PRON'),\n",
       " ('need', 'VERB'),\n",
       " ('to', 'PART'),\n",
       " ('have', 'VERB'),\n",
       " ('some', 'DET'),\n",
       " ('fun', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('life', 'NOUN'),\n",
       " ('.', 'PUNCT')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token.text, token.pos_) for token in tokens_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miejsce na Tw√≥j kod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0J9krYr_qal"
   },
   "source": [
    "### ‚≠ê Zadanie sprawdzajƒÖce umiejƒôtno≈õci‚≠ê\n",
    "\n",
    "**A teraz** zobaczmy, ile i jakich tag√≥w (w tekstach mo≈ºna znale≈∫ƒá te≈º okre≈õlenie POS tag√≥w) mamy w naszych przyk≈Çadach. Jakie problemy mogƒÖ byƒá poruszone przy okazji takiego zadania? Jak zrobiƒá wykres? (Za wykres jest bonus üìä üòÄ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ihx1WDro_wKH"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "pos_quantities = defaultdict(int)\n",
    "for token in tokens_en:\n",
    "  pos_quantities[token.pos_] += 1\n",
    "print(pos_quantities)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(list(pos_quantities.keys()), list(pos_quantities.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAgdEyZI_02h"
   },
   "source": [
    "Lematyzacja\n",
    "Wchodzimy na kolejny poziom abstrakcji. Liczymy teraz nie tokeny, a typy. Za≈Ç√≥≈ºmy, ≈ºe chcemy policzyƒá dzieci z przedszkola bawiƒÖce siƒô g≈Ço≈õno na placu zabaw za naszym oknem. Chcemy dowiedzieƒá siƒô, kto krzyczy g≈Ço≈õniej, dziewczynki czy ch≈Çopcy. W tym celu przyglƒÖdamy siƒô ka≈ºdemu dziecku i stwierdzamy, czy jest ch≈Çopcem czy dziewczynkƒÖ. Sprowadzamy patrzenie na obiekt do binarnego wyboru p≈Çci, czasem mamy wƒÖtpliwo≈õci, co jest normalne. Mo≈ºemy (z wahaniem lub bez) powiedzieƒá, ≈ºe na placu zabaw sƒÖ dwa typy dzieci: krzyczƒÖce i nie, dziewczynki i ch≈Çopcy. I podobnie jest z lematyzacjƒÖ, choƒá trochƒô inaczej. ≈ªeby zobaczyƒá, co jest w zdaniu, musimy przyjrzeƒá siƒô okazom i stwiedziƒá, ≈ºe je≈õli w tek≈õcie dziecko drze siƒô (jakby jutra nie by≈Ço), dar≈Ço siƒô (a≈º do momentu, kiedy nie zag≈Çuszy≈Çy go syreny policyjne) lub bƒôdzie siƒô dar≈Ço (do uko≈Ñczenia 18 roku ≈ºycia), to m√≥wimy o jednej czynno≈õci darcia siƒô wyra≈ºonej jako r√≥≈ºne formy czasownika DRZEƒÜ SIƒò. To jak w s≈Çowniku. Je≈õli chcemy sprawdziƒá pisowniƒô, szukamy czego≈õ co jest w mianowniku liczby pojedynczej i rodzaju mƒôskim, albo w bezokoliczniku, albo w stopniu r√≥wnym i te≈º rodzaju mƒôskim.\n",
    "\n",
    "Co i kiedy liczymy? WypisujƒÖc wszystkie elementy (tokeny), liczyli≈õmy budulec tekstu. WypisujƒÖc lematy (typy), liczymy u≈ºycie konkretnych pojƒôƒá niezale≈ºnie od ich formy.\n",
    "\n",
    "Je≈õli chcesz policzyƒá, ile s≈Ç√≥w zosta≈Ço wymienionych w tek≈õcie, bardzo przydatne jest sprowadzenie wszystkich s≈Ç√≥w do ich form podstawowych. Proces ten nazywany jest lematyzacjƒÖ. Tekst przetworzony za pomocƒÖ spacy zawiera ju≈º lematy dla ka≈ºdego tokena. Wykorzystamy tƒô technikƒô w dalszej czƒô≈õci laboratorium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIEpFin0_3cE"
   },
   "outputs": [],
   "source": [
    "[(token.text, token.lemma_) for token in tokens_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miejsce na Tw√≥j kod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_Y0w9TwHxiq"
   },
   "source": [
    "‚≠ê Zadanie sprawdzajƒÖce umiejƒôtno≈õci ‚≠ê\n",
    "> Dla nietypowych rzeczownik√≥w w jƒôzyku angielskim:\n",
    "\n",
    "* entities\n",
    "* was\n",
    "* mice\n",
    "*cacti\n",
    "* octopi\n",
    "\n",
    "znajd≈∫ lematy i oce≈Ñ, czy spacy rozpozna≈Ç je poprawnie. Czy mo≈ºesz wskazaƒá analogiczne przyk≈Çady dla polskiego?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oPMnY3xpH6ZT"
   },
   "outputs": [],
   "source": [
    "# Miejsce na Tw√≥j kod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mih5AZl5_35c"
   },
   "source": [
    "## Named entity recognition\n",
    "\n",
    "Analiza tekstu przy u≈ºyciu `spacy` mo≈ºe byƒá bardziej zaawansowana (do tej pory analizowali≈õmy sk≈Çadniƒô, teraz czas na odrobinƒô semantyki). Takim bardziej z≈Ço≈ºonym zadaniem jest rozpoznawanie encji nazwanych (NER), a wiƒôc pewnego typu obiekt√≥w, kt√≥re mogƒÖ byƒá nazwƒÖ w≈ÇasnƒÖ, ale wcale nie muszƒÖ.\n",
    "\n",
    "### Zatem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fqAvaZj_Cgiv"
   },
   "outputs": [],
   "source": [
    "ner_result = nlp_en(\"\"\"Israel is being urged by the international community - including close ally the US - to do more to limit civilian casualties.\n",
    "\n",
    "Hamas officials say at least 16,248 people have been killed in Gaza since the start of the conflict, about three quarters of them women and children.\n",
    "\n",
    "Defending Israel's war strategy, former PM Naftali Bennett has told the BBC that Israel has been showing restraint in Gaza.\n",
    "\n",
    "If we wanted to harm civilians, we could have won the whole war in one day on October 8th, he said.\n",
    "\n",
    "We could have indiscriminately bombed Gaza.\n",
    "\n",
    "It could have been the easiest thing in the world... [but] we're not doing that.\"\"\")\n",
    "[(e.text, e.label_, e.start_char, e.end_char) for e in ner_result.ents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONbTGgU-Ex_0"
   },
   "source": [
    "Ka≈ºda kategoria ma w SpaCy rozwiniƒôcie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wGruXB_UEgHn"
   },
   "outputs": [],
   "source": [
    "spacy.explain('GPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Woivn9naE9Al"
   },
   "source": [
    "‚≠ê Zadanie sprawdzajƒÖce umiejƒôtno≈õci ‚≠ê\n",
    "\n",
    "> Zobacz na to samo zadanie, ale w jƒôzyku polskim.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RO0Z_uJ1hNh"
   },
   "source": [
    "#### ‚≠ê Zadanie sprawdzajƒÖce umiejƒôtno≈õci ‚≠ê\n",
    "\n",
    "Znajd≈∫ tekst, kt√≥ry zawiera typ `WORK_OF_ART`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UgPECLjt1j3J"
   },
   "outputs": [],
   "source": [
    "# Miejsce na Tw√≥j kod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0RM3w_wFJsc"
   },
   "source": [
    "### Obrazki\n",
    "Modu≈Ç displacy odpowiada za wizualizacjƒô wynik√≥w dzia≈Çania NERa. Podkre≈õlenie / wyr√≥≈ºnienie kolorystyczne sprawiajƒÖ, ≈ºe ≈Çatwiej odczytaƒá wyniki, ≈ºeby pobie≈ºnie je przeanalizowaƒá."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHBUbGOuFt2y"
   },
   "outputs": [],
   "source": [
    "spacy.displacy.render(ner_result, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xNIdIyLGY0I"
   },
   "source": [
    "KorzystajƒÖc z modu≈Çu `displacy` mo≈ºemy te≈º przeszukiwaƒá informacje pod kƒÖtem jednej wybranej lub kilku po≈ºƒÖdanych kateogrii. ≈ªeby tego dokonaƒá, musimy skorzystaƒá z funkcji `displacy.render`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lr8o6bPUGi5l"
   },
   "outputs": [],
   "source": [
    "spacy.displacy.render(ner_result, style=\"ent\", jupyter=True, options={\"ents\": [\"PERSON\", \"GPE\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7UQMXj7GAaK"
   },
   "source": [
    "#### ‚≠ê Zadanie sprawdzajƒÖce umiejƒôtno≈õci ‚≠ê\n",
    "\n",
    "Spr√≥buj przeanalizowaƒá d≈Çu≈ºszy tekst za pomocƒÖ `spacy` i zwizualizuj wynik NER za pomocƒÖ `displacy`. U≈ºyj jakiego≈õ artyku≈Çu znalezionego w sieci.\n",
    "\n",
    "Nastƒôpnie policz ile razy ka≈ºdy typ encji zosta≈Ç wykryty w tek≈õcie i wy≈õwietl statystyki. Dodatkowy bonus za wykres üìä üòÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jaciXADYZ6Qp"
   },
   "outputs": [],
   "source": [
    "# Miejsce na Tw√≥j kod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPYmmqLqG5YI"
   },
   "source": [
    "## Wykrywanie podobie≈Ñstwa tekst√≥w\n",
    "\n",
    "### Bag of words\n",
    "\n",
    "Do tej pory analizowali≈õmy problem interpretacji formy tekstu, wskazywania konkretnych struktur i przypisywania ich do specyficznej klasy. Teraz zajmiemy siƒô problemem interpretacji znaczenia. Przyjrzyjmy siƒô r√≥≈ºnym tekstom i sprawd≈∫my, jak te teksty sƒÖ do siebie podobne. Dla uproszczenia tekstem bƒôdƒÖ pojedyncze zdania.\n",
    "\n",
    "> The quick brown fox jumps over the lazy dog.\n",
    "\n",
    "> The dog kept barking over the night.\n",
    "\n",
    "> A lazy fisherman with his dog met a fox last night.\n",
    "\n",
    "Przy niedu≈ºej liczbie tekst√≥w jeste≈õmy w stanie naocznie por√≥wnaƒá pr√≥bki i okre≈õliƒá ich podobie≈Ñstwo. Czym jest owo podobie≈Ñstwo? Gdyby≈õmy mieli scharakteryzowaƒá sposoby, na jakie teksty sƒÖ podobne, jakby≈õmy je okre≈õlili?\n",
    "\n",
    "Bardzo czƒôsto stosowanym sposobem na znalezienie zdefiniowanego podobie≈Ñstwa jest technika zwana *bag of words* (https://en.wikipedia.org/wiki/Bag-of-words_model). Polega ona na obliczeniu czƒôstotliwo≈õci wystƒôpowania s≈Ç√≥w we wszystkich tekstach, uporzƒÖdkowaniu tekstu wg najpopularniejszych z nich, a nastƒôpnie przedstawieniu tekstu jako listy liczb ca≈Çkowitych zawierajƒÖcych liczbƒô wystƒÖpie≈Ñ tych s≈Ç√≥w. Co wa≈ºne, to podej≈õcie zupe≈Çnie ignoruje\n",
    "\n",
    "Przyk≈Çad lepszy ni≈º wyk≈Çad!\n",
    "\n",
    "Do obliczenia metryk tekstowych u≈ºyjemy modu≈Çu `sklearn`. Klasa `CountVectorizer` wykonuje wszystkie obliczenia za nas. Parametr `max_features=5` m√≥wi wektoryzatorowi, ≈ºe chcemy wybraƒá co najwy≈ºej 5 najpopularniejszych token√≥w ze wszystkich tekst√≥w.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "myGn2xGKI9hr"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The dog kept barking very loud barking and barking again over the night.\",\n",
    "    \"A lazy fisherman with his dog met a fox last night.\",\n",
    "]\n",
    "\n",
    "count_vector = CountVectorizer(max_features=8)\n",
    "data_count = count_vector.fit_transform(texts)\n",
    "data_count.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMeNodp3Kj8y"
   },
   "source": [
    "OK, co oznaczajƒÖ dane liczbowe, kt√≥re uzyskali≈õmy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0ABs1ziKyDA"
   },
   "outputs": [],
   "source": [
    "count_vector.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1QLo1YXWc2cH"
   },
   "outputs": [],
   "source": [
    "# Miejsce na Tw√≥j kod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fme7G6lDK1Eg"
   },
   "source": [
    "# Tokeny dla angielskiego to:\n",
    "\n",
    "```\n",
    "[ 'barking', 'dog', 'fox', 'over', 'the']\n",
    "```\n",
    "\n",
    "z reprezentacjƒÖ wektorowƒÖ:\n",
    "\n",
    "```\n",
    "array([[0, 1, 1, 1, 2],\n",
    "       [3, 1, 0, 1, 2],\n",
    "       [0, 1, 1, 0, 0]], dtype=int64)\n",
    "```\n",
    "\n",
    "Co oznacza, ≈ºe:\n",
    "* s≈Çowo `barking` pojawia siƒô w og√≥le trzy razy, ale w jednym zdaniu\n",
    "* s≈Çowo `dog` w pierwszym tek≈õcie pojawia siƒô tylko raz, podobnie w trzecim\n",
    "* s≈Çowo `fox` pojawia siƒô raz w pierwszym i raz w trzecim tek≈õcie\n",
    "* s≈Çowo `over` nie wystƒôpuje w pierwszym tek≈õcie\n",
    "* s≈Çowo `the` pojawia siƒô dwukrotnie w pierwszym i drugim tek≈õcie, w trzecim nie wystƒôpuje wcale\n",
    "\n",
    "Now you should understand the *bag of words* text representation. We can say that the more similar the vectors are, the more similar the texts are, too. We can obviously calculate the distance between them and even visualize them on a chart, but we need a few more exercies and obviously - more data!\n",
    "\n",
    "#### ‚≠ê Zadania sprawdzajƒÖce umiejƒôtno≈õci ‚≠ê\n",
    "\n",
    "Przeprowad≈∫ analogiczny eksperyment dla jƒôzyka polskiego i wyciƒÖgnij wnioski podobnie jak w powy≈ºszym ƒáwiczeniu.\n",
    "\n",
    "Zwr√≥ƒá uwagƒô na parametr `max_features` i spr√≥buj zmieniaƒá jego warto≈õƒá, obserwujƒÖc zmianƒô reprezentacji wynik√≥w. WyciƒÖgnij wnioski na temat prze≈Ço≈ºenia warto≈õci parametru na jako≈õƒá prezentowanych wynik√≥w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qe2_07hOMKbO"
   },
   "outputs": [],
   "source": [
    "# Miejsce na Tw√≥j kod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEB6xm4jPzaR"
   },
   "source": [
    "Modele typu transformers korzystajƒÖ z modyfikacji tego podej≈õcia. Wiƒôcej o rozwiƒÖzaniu problemu podobie≈Ñstwa tu: https://huggingface.co/tasks/sentence-similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0EMySonML50"
   },
   "source": [
    "### Stopwords\n",
    "\n",
    "Semantyczne rozwa≈ºania nad jƒôzykiem prowadzƒÖ do obserwacji, ≈ºe s≈Çowa nie przekazujƒÖ por√≥wnywalnie istotnych informacji. Co to znaczy? W przypadku przyk≈Çadu z jƒôzyka angielskiego, s≈Çowo `the` m√≥wi nam nieco mniej ni≈º s≈Çowo `dog` czy `lazy`, a jednak to ono pojawia siƒô w tekstach najczƒô≈õciej. Nie oznacza to oczywi≈õcie, ≈ºe to s≈Çowo nic nie znaczy bƒÖd≈∫ nie pe≈Çni w systemie jƒôzykowym istotnej funkcji. W tym zadaniu rozpatrujemy tylko istotno≈õƒá s≈Çowa w prze≈Ço≈ºeniu na znaczenie ca≈Çego tekstu lub grupy tekst√≥w i przez taki pryzmat bƒôdziemy patrzeƒá na `stopwords`, a wiƒôc s≈Çowa popularne, budulce tekstu, nienosƒÖce znaczenia.\n",
    "`Stopwords` obs≈Çugiwane sƒÖ w pakiecie `SpaCy` przez modu≈Ç `sklearn` (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), kt√≥ry identyfikuje je, a nastƒôpnie usuwa z reprezentacji zbioru danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QkhFQzFdMs7U"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "list(ENGLISH_STOP_WORDS)[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHOezUD9OHnJ"
   },
   "source": [
    "Nie musisz importowaƒá stopwords, aby z nich korzystaƒá, poniewa≈º sƒÖ one zarzƒÖdzane wewnƒôtrznie w pakiecie (`_` w nazwie pakietu).\n",
    "\n",
    "Teraz wszystko, co musisz zrobiƒá, to zdefiniowaƒá wbudowanƒÖ listƒô stopwords, kt√≥rych chcesz u≈ºyƒá przed obliczeniem wektor√≥w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSzlqDvFOSbH"
   },
   "outputs": [],
   "source": [
    "count_vector = CountVectorizer(max_features=10, stop_words = 'english')\n",
    "data_count = count_vector.fit_transform(texts)\n",
    "count_vector.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L52dVF9qO_N3"
   },
   "source": [
    "### Wizualizacja danych\n",
    "\n",
    "\n",
    "Wykrywanie podobnych tekst√≥w w przypadku du≈ºej ilo≈õci danych mo≈ºe stanowiƒá wyzwanie. Zawsze pomocna jest wizualizacja danych na ekranie, wiƒôc mo≈ºemy wykre≈õliƒá wektory i sprawdziƒá, czy mo≈ºemy wykryƒá jakie≈õ grupy na ekranie. Bƒôdzie to trudne w przypadku trzech tekst√≥w, na kt√≥rych obecnie pracujemy, ale zrozumiesz ideƒô.\n",
    "\n",
    "Mo≈ºemy teraz zawiesiƒá to laboratorium i poczekaƒá do 2048 roku, kiedy ekrany 5D bƒôdƒÖ dostƒôpne, lub u≈ºyƒá popularnego algorytmu `t-SNE` do *sp≈Çaszczenia* danych, a nastƒôpnie ich wizualizacji. Wybierzemy drugie rozwiƒÖzanie üòâ.\n",
    "\n",
    "Nie zag≈ÇƒôbiajƒÖc siƒô zbytnio w dzia≈Çanie tego algorytmu, jest on w stanie zredukowaƒá wektory XD do wektor√≥w YD, z X>Y, zachowujƒÖc odleg≈Ço≈õci miƒôdzy nimi. W przypadku naszego tekstu chcemy zredukowaƒá wektory 5D (5 cech tekstu) do wektor√≥w 2D (czyli do formatu, kt√≥ry mo≈ºna wykre≈õliƒá na ekranie).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6c2W1S_QGIr"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne_model = TSNE(n_components=2, perplexity=2)\n",
    "tsne_data = tsne_model.fit_transform(data_count.toarray())\n",
    "\n",
    "tsne_data\n",
    "#data_count.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zn5GI1iFQtr1"
   },
   "source": [
    "Algortym zredukowa≈Ç wektory, co mo≈ºemy zobaczyƒá, zamiast musieƒá sobie to wyobra≈ºaƒá."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbwqxktaQ0gd"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(tsne_data[:, 0], tsne_data[:, 1])\n",
    "\n",
    "for i, label in enumerate([\"quick fox\", \"barking dog\", \"lazy fisherman\"]):\n",
    "    ax.annotate(label, (tsne_data[i, 0], tsne_data[i, 1]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I60UgASRT70i"
   },
   "source": [
    "IstniejƒÖ tylko trzy punkty danych, wiƒôc trudno powiedzieƒá, czy teksty mo≈ºna uznaƒá za podobne do siebie, czy nie. Gdyby≈õmy jednak mieli znacznie wiƒôcej tekst√≥w, mogliby≈õmy podejrzewaƒá, ≈ºe punkty danych utworzy≈Çyby pewne rozr√≥≈ºnialne grupy, co oznacza, ≈ºe teksty m√≥wiƒÖ o podobnych tematach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bvhm1-manrMN"
   },
   "source": [
    "## Zbiory danych\n",
    "\n",
    "Do ostatniego zadania potrzebujemy wiƒôcej danych, ≈ºeby zaobserwowaƒá proces klasteryzacji. JednƒÖ z mo≈ºliwych dr√≥g pozyskania danych jest skorzystanie z modu≈Çu [HuggingFace](https://huggingface.co/docs/datasets/v1.8.0/loading_datasets.html) `datasets`, aby pobraƒá kilka tekst√≥w, nad kt√≥rymi mo≈ºemy pracowaƒá.\n",
    "\n",
    "Zobaczmy, co jest w ≈õrodku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CU9xQv2RoAs-"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "datasets.list_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wyhw7igoQdD"
   },
   "source": [
    "Zbior√≥w danych jest sporo, a ich liczba stale ro≈õnie. Na potrzeby tego eksperymentu wybierzmy dowolny zbi√≥r (zadanie nie jest tak wyspecyfikowane, by nak≈Çadaƒá na nas konieczno≈õƒá wyboru wg konkretnych kryteri√≥w).\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8GgJ05UnoclR"
   },
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('ag_news', split='train')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oU8czV0vqBkG"
   },
   "source": [
    "Jak widzieli≈õmy w poprzednich przyk≈Çadach, lista tekst√≥w bƒôdzie na razie ≈ÇatwiejszƒÖ strukturƒÖ do pracy. MajƒÖc powy≈ºszy zbi√≥r danych z polami `text` i `label`, mo≈ºemy utworzyƒá listƒô tekst√≥w z prostym zrozumieniem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GyDWshMqSHq"
   },
   "outputs": [],
   "source": [
    "large_texts = [item['text'] for item in dataset]\n",
    "large_texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYYikmVXUM9Y"
   },
   "source": [
    "## ‚≠ê Zadanie sprawdzajƒÖce umiejƒôtno≈õci üóª ‚≠ê\n",
    "\n",
    "Masz wszystkie narzƒôdzia!\n",
    "\n",
    "Zbierz du≈ºy zbi√≥r danych tekst√≥w z *HF* i:\n",
    "\n",
    "1.   Przygotuj je do analizy, np.\n",
    "  \n",
    "  a. Stokenizuj je.\n",
    "\n",
    "  b. Przekszta≈Çƒá tokeny w lematy (tak, aby `dog` i `dogs` by≈Çy traktowane jako ta sama funkcja).\n",
    "2. Przedstaw teksty jako bag of words, pamiƒôtajƒÖc o stopwords. Eksperymentuj z liczbƒÖ cech. Je≈õli oka≈ºe siƒô, ≈ºe istniejƒÖ cechy, kt√≥re wp≈ÇywajƒÖ na reprezentacjƒô, wr√≥ƒá do kroku 1. i we≈∫ to pod uwagƒô podczas przygotowywania danych.\n",
    "3. Zwizualizuj dane na wykresie (bez etykiet dla lepszej wydajno≈õci). Czy mo≈ºesz wyr√≥≈ºniƒá jakie≈õ grupy tekst√≥w? O czym sƒÖ te teksty?\n",
    "4. Wykryj nazwane jednostki w reprezentantach grup. Czy nazwane jednostki sugerujƒÖ r√≥wnie≈º temat tekstu?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dkIYNxKC4D0c"
   },
   "outputs": [],
   "source": [
    "# Miejsce na Tw√≥j kod"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
