{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faac065c-ab39-45cd-aa8e-3a521dddb9ed",
   "metadata": {},
   "source": [
    "### Multiword Expressions\n",
    "\n",
    "\n",
    "Wydzielaliśmy do tej pory słowa w izolacji, choć wiemy, że występują one grupami. Jak stwierdzić, czy to, co obserwujemy w tekście to przypadkowa konstrukcja (nazwy rzeczy wymieniane jednym tchem, występujące obok siebie takie a nie inne słowa) czy może stałe połączenie wyrazowe, które możemy badać jako zjawisko (na wiele sposobów!) lub wykluczyć z korpusu, który poddajemy badaniom. \n",
    "\n",
    "Trwałe połączenia wyrazowe to frazy, które znamy jako przysłowia, powiedzonka, nazwy, złożenia. Więcej o nich i ich istocie można poczytać tu: https://en.wikipedia.org/wiki/Multiword_expression. Warto jednak zaznaczyć, że koncepcja jednostki wielowyrazowej ulega modyfikacjom w zależności od obranego paradygmatu badawczego! Dobrze jest przemyśleć sobie, co rozumiemy przez tę konstrukcję i konsekwentnie stosować się do tej wizji w naszych badaniach.\n",
    "\n",
    "Przedmiotem tego laboratorium jest koncept samego poszukiwania jednostek wielowyrazowych, ich opisu gramatycznego, zestawienia częstych z rzadkimi wystąpieniami w korpusie.\n",
    "\n",
    "W ramach wykrywania bigramów, będziemy kolejno:\n",
    "1. Pobierać i tokenizować korpus FIQA.\n",
    "2. Obliczać liczbę wystąpień złożeń dwóch elementów w tekście. Dla znanego przez nas zdania: The quick brown fox jumped over lazy dog rozłożenie takich dwuelementowych segmentów wygląda następująco:\n",
    "\"the quick\": 1\n",
    "\"quick brown\": 1\n",
    "\"brown fox\": 1\n",
    "...\n",
    "\"dog .\": 1\n",
    "3. Wyselekcjonujemy te segmenty, które tworzone są tylko przez słowa (ciekawość badacza: dlaczego teraz? dlaczego działamy w taki sposób?)\n",
    "4. Użyjemy miary Pointwise Mutual Informatio (PMI), by obliczyć siłę powiązań pomiędzy elementami tworzącymi segment. PMI (https://en.wikipedia.org/wiki/Pointwise_mutual_information) to tylko jeden ze sposobów na otrzymanie takiej informacji, inne opisano tu: https://onlinelibrary.wiley.com/doi/full/10.1111/lang.12225.\n",
    "5. Znajdziemy najczęstsze segmenty występujące w tekście. Określimy też próg przypadkowych połączeń wyrazów jako 5 wystąpień, nie będzie nas interesowało nic, co występuje 5 razy i rzadziej.\n",
    "6. Spojrzymy na cały problem z punktu widzenia składni, a więc oznaczymy segmenty, przypisując im interpretację morfologiczną, policzymy, które złożenia są najczęstsze (ciekawość badacza: jak najczęstrze kategorie morfologiczne mają się w porównaniu do naszych najczęstszych słów z wcześniejszych punktów?\n",
    "\n",
    "Zaczynamy od przygotowania środowiska."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3ff21a-3cbf-47cc-b5c5-e49bce377ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "from datasets import load_dataset\n",
    "from spacy.lang.pl import Polish\n",
    "\n",
    "##!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b906457c",
   "metadata": {},
   "source": [
    "Użyte moduły:\n",
    "- `string` https://docs.python.org/3/library/string.html\n",
    "- `collections` https://docs.python.org/3/library/collections.html\n",
    "- `itertools` https://docs.python.org/3/library/itertools.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac9a34f-c0ab-4929-b669-9de31cb20ab3",
   "metadata": {},
   "source": [
    "## Zadanie 1\n",
    "\n",
    "Pobieramy korpus, tokenizujemy go, używając do tego znanych już narzędzi i zasobów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02a10a6-e53c-4d04-a883-889a5ad7991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('clarin-knext/fiqa-pl', 'corpus')['corpus']['text']\n",
    "dataset = ' '.join(dataset)\n",
    "\n",
    "nlp = Polish()\n",
    "tokenizer = nlp.tokenizer\n",
    "\n",
    "tokens = tokenizer(dataset)\n",
    "tokens = list(map(lambda t: t.text.lower(), tokens))\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d08493-35b0-415e-9a8f-7530673f64d7",
   "metadata": {},
   "source": [
    "## Zadanie 2-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6f7b6b-afff-4050-90f6-ec5187378428",
   "metadata": {},
   "source": [
    "Obliczamy bigramy:\n",
    "(tu sprytne wyjaśnienie: https://www.exploredatabase.com/2020/04/bigram-probability-estimation-of-word-sequence-example.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72631447-0003-41f7-bf57-ec920503c6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = [a + ' ' + b for a, b in zip(tokens, tokens[1:])]\n",
    "bigrams = list(filter(lambda x: len(x.split(' ')) == 2, bigrams))\n",
    "bigrams_counter = Counter(bigrams)\n",
    "\n",
    "for i, (key, val) in zip(range(1, 50), bigrams_counter.items()):\n",
    "    print(f'{i}. {key}: {val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c294487-52c3-4b98-8dae-42db9c8b88a3",
   "metadata": {},
   "source": [
    "I filtrujemy bigramy, aby zachować tylko takie, które zawierają wyrazy składające się wyłącznie z liter polskiego alfabetu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc8a2c1-1229-4aba-b2a6-be071a811e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discard_elements(iterable, filter_fn):\n",
    "    discard = []\n",
    "    \n",
    "    for element in iterable:\n",
    "        if filter_fn(element):\n",
    "            discard.append(element)\n",
    "            continue\n",
    "    \n",
    "    for d in discard:\n",
    "        del iterable[d]\n",
    "\n",
    "\n",
    "all_chars = string.ascii_letters + 'ęóąśłżźćńĘÓĄŚŁŻŹĆŃ '\n",
    "filter_not_letters = lambda element: any([c not in all_chars for c in element])\n",
    "\n",
    "discard_elements(bigrams_counter, filter_not_letters)\n",
    "\n",
    "for i, (key, val) in zip(range(1, 50), bigrams_counter.items()):\n",
    "    print(f'{i}. {key}: {val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d804a3-3722-40e9-8f50-e20ade5d76c7",
   "metadata": {},
   "source": [
    "## Zadanie 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d84bd48-a252-43fa-9c69-ab3461158162",
   "metadata": {},
   "source": [
    "Używamy PMI, żeby stwierdzić, jakie jest prawdopodobieństwo współwystępowania słów z bigramu w korpusie - PMI przybliża siłę powiązania słów. Im wyższy wynik, tym mocniej powiązane są słowa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c885a08e-d2b4-4c0b-9c4d-2dde5d53c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_counter = Counter(tokens)\n",
    "\n",
    "\n",
    "def pmi(bigram):\n",
    "    x, y = bigram.split(' ')\n",
    "    p_x = tokens_counter[x] / len(tokens_counter)\n",
    "    p_y = tokens_counter[y] / len(tokens_counter)\n",
    "    p_xy = bigrams_counter[bigram] / len(bigrams_counter)\n",
    "\n",
    "    if p_x * p_y == 0 or p_xy == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.log2(p_xy / (p_x * p_y))\n",
    "\n",
    "\n",
    "bigrams_pmi = [(pmi(bigram), bigram) for bigram in bigrams_counter]\n",
    "bigrams_pmi[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39095ead-daeb-4bf5-b8c1-2e03dfc6d1da",
   "metadata": {},
   "source": [
    "## Zadanie 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db24d13d-25d1-4e1a-b1c9-bde9050b8c32",
   "metadata": {},
   "source": [
    "Bezpośrednie zastosowanie PMI na korpusie zwraca dość charakterystyczne bigramy, które wyglądają, jakby występowały w korpusie bardzo rzadko (zarówno bigramy, jak i tworzące je tokeny) i dlatego znalazły się na początku rankingu (przez bardzo małe prawdopodobieństwa $p_x, p_y$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5479af8-f572-43b8-9084-4d97acccaab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(bigrams_pmi, reverse=True)[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931ed989-7e2a-49f6-b6fe-50734af0ae70",
   "metadata": {},
   "source": [
    "Odfiltrowanie najrzadszych bigramów pozwala uzyskać bardziej rozsądne wyniki - większość naszych aktualnych wyników to charakterystyczne określenia składające się z dwóch słów (np. \"klęska żywiołowa\", \"książeczka czekowa\") lub nazwy własne (np. \"Stucco Veneziano\", \"Bert Hellinger\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f369f21a-fb74-4001-b5fc-c8d389a9c380",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_rare = lambda element: bigrams_counter[element] < 5\n",
    "discard_elements(bigrams_counter, filter_rare)\n",
    "\n",
    "bigrams_pmi = [(pmi(bigram), bigram) for bigram in bigrams_counter]\n",
    "top_bigrams_pmi = sorted(bigrams_pmi, reverse=True)[:50]\n",
    "top_bigrams_pmi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd20f5c-b0e7-4840-bb22-231d60a10298",
   "metadata": {},
   "source": [
    "## Zadanie 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df6bddf",
   "metadata": {},
   "source": [
    "Na wyniki patrzymy teraz z innej perspektywy - chcemy zobaczyć, jak najczęstsze segmenty rozkładają się w ujęciu gramatycznym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd1165f-7249-430f-824a-3afb68cf612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pl_core_news_sm')\n",
    "\n",
    "dataset = load_dataset('clarin-knext/fiqa-pl', 'corpus')['corpus']['text']\n",
    "dataset = map(nlp, dataset)\n",
    "dataset = list(map(lambda word: f'{word.lemma_}:{word.tag_}'.lower(), chain.from_iterable(dataset)))\n",
    "\n",
    "for i, word in enumerate(dataset[:10], 1):\n",
    "    print(f'{i}. {word}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a8c200-2644-4993-88c8-1686f692f59e",
   "metadata": {},
   "source": [
    "Ranking top-10 wartości PMI obliczony na korpusie po lematyzacji oraz tagowaniu daje bardzo podobne wyniiki - pojawiają się w nim te same lub należące do tych samych kategorii bigramy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794c1ded-bb67-475f-a9cf-6e5baf10c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = [a + ' ' + b for a, b in zip(dataset, dataset[1:])]\n",
    "bigrams = list(filter(lambda x: len(x.split(' ')) == 2, bigrams))\n",
    "bigrams_counter = Counter(bigrams)\n",
    "tokens_counter = Counter(dataset)\n",
    "\n",
    "def filter_not_letters(element):\n",
    "    a, b = element.split(' ')\n",
    "    a, b = a.split(':')[0], b.split(':')[0]\n",
    "    return any([c not in all_chars for c in a + b])\n",
    "\n",
    "discard_elements(bigrams_counter, filter_not_letters)\n",
    "discard_elements(bigrams_counter, filter_rare)\n",
    "\n",
    "for i, (key, val) in zip(range(1, 50), bigrams_counter.items()):\n",
    "    print(f'{i}. {key}: {val}')\n",
    "\n",
    "print()\n",
    "\n",
    "bigrams_pmi = [(pmi(bigram), bigram) for bigram in bigrams_counter]\n",
    "top_bigrams_pmi_tag_lem = sorted(bigrams_pmi, reverse=True)[:10]\n",
    "top_bigrams_pmi_tag_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8df42a-9e75-46ac-93ec-45ab8183b30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = defaultdict(list)\n",
    "\n",
    "for bigram, count in bigrams_counter.items():\n",
    "    a, b = bigram.split(' ')\n",
    "    a, b = a.split(':')[1], b.split(':')[1]\n",
    "    groups[(a, b)].append((count, bigram))\n",
    "\n",
    "for i, key in zip(range(1, 11), groups):\n",
    "    print(f'{i}. {key}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b388d-08c4-44ab-82c7-90e31d7276ab",
   "metadata": {},
   "source": [
    "## Podsumowanie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a014bd6-7677-4413-b1ef-c03ce133acf6",
   "metadata": {},
   "source": [
    "Kategorie najczęściej spotykanych bigramów są dość intuicyjne, jeśli chodzi o język naturalny, wyniki są zgodne z regułami łączenia słów w języku polskim:\n",
    "\n",
    "- przyimek + rzeczownik (np. \"na przykład\"),\n",
    "- przymiotnik + rzeczownik (np. \"druga strona\"),\n",
    "- rzeczownik + przyimek (np. \"podatek od\"),\n",
    "- rzeczownik + rzeczownik (np. \"miejsce pracy\"),\n",
    "- przyimek + przymiotnik (np. \"w którym\"),\n",
    "- rzeczownik + przymiotnik (np. \"karta kredytowa\"),\n",
    "- rzeczownik + czasownik (np. \"to jest\"),\n",
    "- przeczenie + czasownik (np. \"nie jest\"),\n",
    "- czasownik + rzeczownik (np. \"oznacza to\"),\n",
    "- rzeczownik + spójnik (np. \"pieniądz i\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4db0056-59ba-4f10-8b36-ad8da0083073",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_count = [(sum(map(lambda g: g[0], val)), key) for key, val in groups.items()]\n",
    "groups_count = sorted(groups_count, reverse=True)\n",
    "groups_count[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea77412-2a2a-42c2-bad4-152554df7455",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, group in groups_count[:10]:\n",
    "    print('-' * 50)\n",
    "    print(group)\n",
    "    \n",
    "    for i, v in enumerate(sorted(groups[group], reverse=True)[:5], 1):\n",
    "        print(f'{i}. [{v[0]}] {v[1]}')\n",
    "        \n",
    "print('-' * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
